---
title: "Modeling Wheat Yields in India: Describing & Exploiting Spatio-Temporal Variability with Panel Regression"
bibliography: MyCollection.bib
output: word_document
---

```{r setup, include=FALSE}
# citation styles can be managed here: http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
# http://rmarkdown.rstudio.com
# http://rmarkdown.rstudio.com/authoring_pandoc_markdown.html#headers
setwd('H:/Projects/India_Index_Insurance/India_Index_Insurance_Code/WriteUp/')
source('H:/Scripts/multi_grep_character.R')
library(pander)
library(knitr)
library(plyr)
library(ggplot2)
library(plm) # works on desktop at school
library(stargazer)
library(reshape)
library(stats)
library(rms)
library(english)
library(splm)
library(spdep)
library(rgdal)

yield_ndvi = read.csv('..//yield_ndvi.csv')
focus_group = read.csv('Focus Groups Summary.csv',stringsAsFactors = F)
Table_number = 1
Figure_number = 1
Appendix_Table_number =1
Appendix_Function_number =1
Formula_number = 1

```

#Introduction

The ability to monitor and predict crop yields in developing countries is critical to the successful adaptation to changes in our climate. Increased temperatures and variability has already been linked to losses in maize and wheat yields (-3.8 and 5.5% respectively)and crop prices globally [@Lobell616]. Although much effort has been placed on modeling the spatial distribution of these shifts, less effort has been placed on how yields vary across space and time [@Ray2015]. Advances in remote sensing provide new avenues to monitor agricultural crop health at high spatial and temporal resolution. However, our ability to monitor changes in plant productivity is still limited in the more complex environments  common to many developing countries [@Mann2015]. 

One primary thrust of these efforts has evolved out of the index insurance space. Index insurance is...   

The main objectives of this project is to apply and compare statistical methods commonly applied these problems outside of the field of geography. In particular we will focus on the application of spatial panel regression to monitor spatio-temporal variability in wheat yields for Punjab and Haryana India.

#Methods
##Overview and Study Area

We examine wheat yields at the district level for Punjab and Haryana India for Rabi season (roughly Nov-Apr) for the period of `r paste(range(yield_ndvi$years, na.rm=T),collapse=' to ')`. Both Punjab and Haryana are extensively cropped but is comprised of a large number of smaller heterogeneous plots. Both states are also extensively double-cropped with rice planting in the Kharif season (roughly May-Oct) and Wheat planted in the Rabi season. Rabi season wheat yield range from `r paste(range(yield_ndvi$yield_tn_ha, na.rm=T),collapse=' to ')` metric tons per hectare (Table `r paste(Table_number)`, Figure `r paste(Figure_number)`).

*Figure `r paste(Figure_number)`: Mean Rabi Season Wheat Yields Metric Tons per Hectare by District `r Figure_number =Figure_number+1 `*
![Test](H:\Projects\India_Index_Insurance\India_Index_Insurance_Code\WriteUp\Yield_tn_ha.png)

 
Here we develop a (non)spatial panel regression model to estimate wheat output per hectare using the open-source programming language R. This model utilizes historical data on plant phenology statistics obtained from the Moderate Resolution Imaging Spectroradiometer (MODIS) satellite. The objective is to develop a handful of metrics that can be used to accurate predict inter-annual variability in wheat yields at the district level. 

##Data
The full model is comprised of `r paste(length(multi_grep_character(c('VEG','dates','EVI','NDVI'), names(yield_ndvi))))` indicators of plant phenology. District level statistics are then generated from pixel level plant indicators.

###Focus Group Interviews
To help better characterize the physical properties and identify challenges, field visits and focus group interviews were conducted in the winter of 2015. These interviews were conducted in 12 villages with 71 participants in Haryana and Pubjab states, in person with International Food Policy Research Institute (IFPRI) staff.  Questions focused on farm characteristics, adopted technologies, Rabi crop calendar dates, and identifying the timing of risks to crops. 

###Remote Sensing Data
Considering the relatively small scale of agriculture in this region (median plot sizes of `r paste(mean(13.5,10))` acres, range of 2 to 17.2 acres) reported during focus groups [@Robles2015], we utilized 250m vegetation products from the MODIS satellites. Vegetation indices are obtained from two 16 days MODIS products (MOD13Q1, MYD13Q1) from the Aqua and Terra satellites [@Didan2006]. Due to the staggered nature of acquisition these products are treated as partially overlapping windows representing 8 days periods [@doraiswamy2007crop]. 

In particular we examine both the predictive power of the Enhanced Vegetation Index (EVI) and the Normalized Difference Vegetation Index (NDVI) **EVI AND NDVI**. Both are sensitive to the amount of chlorophyll in any given pixel and are commonly used to estimate plant productivity and health in agricultural applications [@Mann2015]. After removal of snow, cloud and other flagged low quality cells, we remove all non-agricultural cells through the use of the 500m MODIS land cover product (MCD12Q1) for the appropriate year [@Friedl2010]. The difference in resolutions is expected to have a minimal effect in this case because the extent of rural agriculture in these areas is extremely large. Therefore any cells include or excluded by omission or commission as agriculture should have a minimal effect at the district level. Moreover agricultural patterns are generally uniform over broad areas of Punjab and Haryana. 
 
In addition to cloud cover MODIS data products suffer from four additional sources of error including atmospheric interference, georeferencing, bidirectional reflectance effect and differences in day of the year each pixel is observed [@doraiswamy2007crop]. While indices such as NDVI minimize the effects of atmospheric distortion but will directly influence indices values. To minimize the effects of the artifacts described above we test the use of temporal smoothing splines and outlier removal  [@hastie1990generalized]  **REFERENCE JOSHUA GRAY**. A visual example of the effects of the cubic smoothing and outlier removal procedure can be seen in Figure `r paste(Figure_number)`. 

*Figure `r paste(Figure_number)`: (Un)smoothed 8-Day NDVI time signature for a dual cropped pixel in Punjab `r aa=Figure_number` `r Figure_number =Figure_number+1 `*
```{r, echo=FALSE,warning=F,fig.width=7, fig.height=5}
 rects = read.csv('./FigureTableData/rects.csv')
 rects$xend = as.Date(strptime(rects$xend,'%Y-%m-%d'))
 rects$xstart = as.Date(strptime(rects$xstart,'%Y-%m-%d'))
 plotdata = read.csv('./FigureTableData/NDVI_smooth_timeseries.csv')
 plotdata$Dates = as.Date(strptime(plotdata$Dates,'%Y-%m-%d'))
 harvest_lines = as.Date(strptime(read.csv('./FigureTableData/harvest_lines.csv',stringsAsFactors = F)[,2],'%Y-%m-%d'))
 plant_lines = as.Date(strptime(read.csv('./FigureTableData/plant_lines.csv')[,2],'%Y-%m-%d'))
 PlantHarvest =  read.csv('./FigureTableData/PlantHarvest.csv') 
 PlantHarvest$planting = as.Date(strptime(PlantHarvest$planting,'%Y-%m-%d'))
 PlantHarvest$harvest = as.Date(strptime(PlantHarvest$harvest,'%Y-%m-%d'))
 max_lines = as.Date(strptime(read.csv('./FigureTableData/max_lines.csv')[,2],'%Y-%m-%d'))

 ggplot()+geom_rect(data = rects, aes(xmin = xstart, xmax = xend,
        ymin = -Inf, ymax = Inf), alpha = 0.4)+
        geom_point(data= plotdata, aes(x=Dates,y=NDVI,group=Legend,colour=Legend),size=2)+
        geom_vline(colour='darkgreen',xintercept = as.numeric(as.Date(strptime(plant_lines,'%Y-%m-%d'))))+
        geom_vline(colour='purple',xintercept = as.numeric(as.Date(strptime(harvest_lines,'%Y-%m-%d'))))+
        geom_vline(colour='red',xintercept = as.numeric(as.Date(strptime(max_lines,'%Y-%m-%d'))))+
        annotate("text", x =(PlantHarvest$planting[1]+61), y = 0.37, label = "Wheat")+
        annotate("text", x =(PlantHarvest$harvest[1]+93), y = 0.31, label = "Rice")+
        coord_cartesian(xlim = (as.Date(c("2010-01-01", "2015-04-01"))))
 
 # windows() used in presentation 
 #  ggplot()+geom_rect(data = rects, aes(xmin = xstart, xmax = xend,
 #        ymin = -Inf, ymax = Inf), alpha = 0.4)+
 #        geom_line(data= plotdata[plotdata$Legend=='NDVI',], aes(x=Dates,y=NDVI,group=Legend),size=2)+
 #        coord_cartesian(xlim = (as.Date(c("2013-01-01", "2015-04-01"))))
 
```
*Time series of NDVI unsmoothed (red) and smoothed with outlier removal (blue) for both crop seasons of 2002 to 2016. Wheat growing season highlighted in dark grey, rice growing season in light grey. Date of estimated Rabi season maximum (red vertical line), estimated greenness onset (green vertical line), estimated harvest date (purple veritcal line).*

To test for model improvements obtained through time series cubic smoothing we iteratively run a regression where the model described below (estimating wheat yields) with varying degrees of smoothing of inputs. For each iteration the degree of smoothing is increased and the adjusted $\ R^2$ and root mean squared error divided by mean yields is reported. 

###Agricultural Survey Data
Agricultural survey data at the district level was obtained from _________. INCLUDE DESCRIPTION OF SWAPPING PUNJAB STATE DATA.

##Summarizing Time: Summarizing Remotely Sensed Data
One of the primary challenges in utilizing an 8 day time series to estimate annual wheat yields is the mismatch in observations. Properties of the time signature must be obtained to characterize and identify important components of the plant phenology time signature correlated with wheat yields in these agricultural systems. Here we utilize `r paste(length(multi_grep_character(c('VEG','dates','EVI','NDVI'), names(yield_ndvi))))` metrics to summarize phenology. These measures take two primary forms: first, growing season statistics, spanning the estimated planting date of wheat (mean DOY:`r paste(round(mean(yield_ndvi$plant_dates,na.rm=T)))`) until harvest date (mean DOY:`r paste(round(mean(yield_ndvi$harvest_dates,na.rm=T)))`); and second monsoon season statistics, spanning end of the Rabi wheat season to end of the Kharif growing season. Two classes of statistics are estimated for these two periods: first, summary statistics (e.g. mean, max, variance) and integration (i.e. integration). 

###Growing Season Metrics
Planting and harvest dates of are estimated for each growing season of interest. These dates are estimated through an iterative search algorithm finding the date of the global minimum NDVI **or EVI** value nearest to the *a priori* estimated date. A priori values were obtained from the focus group interviews described above. For wheat sowing dates were reported to typically start in the last week of October, and harvest to begin in the 2nd week of April. For details on this function see `r A=Appendix_Function_number` `r paste(A)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix below. Basic growing season summary statics including minimum, maximum, mean, and standard deviation can be calculated using function `r B=Appendix_Function_number` `r paste(B)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix below. 

To estimate the cumulative impact of high or low vegetation indices across as season we calculate a variety of integration metrics. These include area under the curve (AUC) of the growing season, the AUC of the increasing portion of the curve (from estimated planting date to growing season maximum), and the AUC of the declining portion of the curve (from growing season maximum to estimated harvest date). For comparision, these values are calculate using two methods, the first using integration using smoothing splines (appendix formula `r C=Appendix_Function_number` `r paste(C)` `r Appendix_Function_number=Appendix_Function_number+1`) and second using trapeziodal estimation (appendix formula `r D=Appendix_Function_number` `r paste(D)` `r Appendix_Function_number=Appendix_Function_number+1`). 

We develop a series of metrics to test if modeled yields could be improved through comparisons to 'ideal' years. This includes calculating the 95th percentile (based on sample quantile where the resulting quantile estimates are approximately median-unbiased regardless of the distribution of x [@hyndman1996sample]) of all NDVI **EVI** values, of maximum values, and of the integral (area under the curve) of NDVI **EVI** values.  These use the built in functionality of R's base stat function show formula `r E=Appendix_Function_number` `r paste(E)` `r Appendix_Function_number=Appendix_Function_number+1`) in the appedix. 

Additional functions were developed to extract the timing of particular phenomena, for instance the date of the maximum value of NDVI **EVI**. Figure `r paste(aa)` visually demonstrates the ability of this function to estimate the timing on greeness onset (referred to henceforth as planting date), seasonal maximums, and harvest dates. For details on these calculations see formula `r F=Appendix_Function_number` `r paste(F)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix. Mutiway ties are handled by perfering the middle most date or if an even number of ties the left middle most date. Another calculates the average value of NDVI **EVI** for each day of the year, which can be used for graphing anomolies (see formula `r G=Appendix_Function_number` `r paste(G)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix). Finally, some of the above codes have improved performance when run on smoothed time series while removing outliers. For this procedure we use a function developed by Joshua Gray at North Carolina State University (see function `r H=Appendix_Function_number` `r paste(H)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix).

###Annual Metrics
Basic annual summary statics including minimum, maximum, mean, and standard deviation can be calculated using function `r I=Appendix_Function_number` `r paste(I)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix below. Alternatively most functions described above can be used to calculate annual vegetation metrics. 

##Summarizing Space & Time: Extraction and Aggregation of Remotely Sensed Data
One major hurdle for this study was the rapid extraction of raster values bases on vector data while maintaining meaningful time stamps. In response we developed the function *extract_value_point_polygon* `r J=Appendix_Function_number` `r paste(J)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix to enhance the performance of the the default raster::extract() function . User processing times were better that 1/6000th that of extract() with the use of a 16-core linux server. Additionally the function can take a list of adjacent rasters to perform data extraction, thereby avoiding vector datasets that span more than the extent of one raster. 
 
*Figure `r paste(Figure_number)`: Benchmark test comparing raster::extract() and the new parrallelized extract functions`r Figure_number =Figure_number+1`*

```{r, echo=FALSE}
bench = read.csv('FigureTableData/benchmark_extract.csv',stringsAsFactors = F)
bench_extract_time = bench[1,2] # remove text data
bench = bench[2:dim(bench)[1],]
bench$cores = as.numeric(bench$cores)
ggplot(data=bench,aes(x=cores,y=times,group=NULL))+geom_line(size=1)+geom_hline(yintercept=bench_extract_time,colour='red',size=1)+xlab('Cores')+ylab('User Time')+ annotate("text", x = 7, y = 150, label = "extract_value_point_polygon")+ annotate("text", x = 7, y = 287, label = "raster::extract", col='red')
```



```{r, echo=FALSE}
# not sure we include this one 
 Neighborhood_quantile=function(extr_values,PlantHarvestTable,Quant_percentile=0.05,num_workers=5,spline_spar = 0){
     # take in values from extract_value_polygon and returns quantile for all raster values wihtin poly
     # if spline_spar = 0, doesn't smooth data, as spline_spar increases smoothing decreases

     # iterate between spatial objects
     registerDoParallel(num_workers)
     result_summary=foreach(i = 1:length(extr_values),.packages='raster',.inorder=T) %dopar%{
        if(is.na(extr_values[[i]])){ print('Empty Object');return(NA)} # avoid empties

        # Get dates from stack names
        dats = strptime( gsub("^.*X([0-9]+).*$", "\\1", names(extr_values[[i]])),format='%Y%j')
        # Calculate smoothed values
        if(spline_spar!=0){
        smooth = lapply(1:dim(extr_values[[i]])[1],function(z){SplineAndOutlierRemoval(
            x = as.numeric(extr_values[[i]][z,]),
            dates=as.Date(dats),
            pred_dates=as.Date(dats),spline_spar)})}else{
            smooth = lapply(1:dim(extr_values[[i]])[1],function(z) as.numeric(extr_values[[i]][z,]))    }
	# calculate quantile for all values over polygon 
        N_Qnt = quantile(x = unlist(smooth),p=Quant_percentile,type=8,na.rm=T)
        N_Qnt     
      }
    result_summary
 }
```


##Exploiting Time: (Spatial) Panel Regression Methods and Models
###Input Data
A summary table of the value of the dependent variable and all independent variables can be found below:
*Table `r paste(Table_number)`: Variance inflation factor summary table `r Table_number =Table_number+1 `*

```{r Input variable summary table, echo=FALSE}
  panderOptions('table.split.table', Inf)
  panderOptions('big.mark', ",")
  panderOptions('keep.trailing.zeros', F)
  table.yield_ndvi = yield_ndvi[,10:dim(yield_ndvi)[2]]
  table.yield_ndvi.out = data.frame(Mean = sapply(table.yield_ndvi, mean, na.rm=T),SD=sapply(table.yield_ndvi, sd, na.rm=T) )
  pander(table.yield_ndvi.out, justify = c('left', 'center', 'center'))
```

###Panel Regression Methods and Diagnostic results
```{r Panel: Read in data, message=FALSE, warning=FALSE, include=FALSE}
###########################################
# regressions on yields
  yield_evi = read.csv('H://Projects/India_Index_Insurance/India_Index_Insurance_Code/yield_ndvi.csv',stringsAsFactors = F)
  # yield_evi = read.csv('C://Users/mmann/Downloads/yield_ndvi (1).csv')
  yield_evi = yield_evi[!is.na(yield_evi$years),]
  
  yield_evi[yield_evi$i ==26 &yield_evi$years==2006,][2,] =NA
  yield_evi = yield_evi[!is.na(yield_evi$year),]
  yield_evi[ yield_evi$yield_tn_ha<1 | yield_evi$yield_tn_ha>6,'yield_tn_ha']= NA 
  
  # IMPORTANT: ORDER TO AVOID PROBLEMS WITH INDEX LATER  - plm sorts by name and year 
  yield_evi=yield_evi[with(yield_evi, order(district, years)), ]
  
```  

#### Spatial Autocorrelation
```{r Panel: Spatial autocorrelation, echo=FALSE, message=FALSE, warning=FALSE}
# run hausman test
  formula2 = yield_tn_ha ~plant_dates+harvest_dates+
    VEG_annual_max_95th_prct+VEG_annual_AUC_95th_prct+VEG_growing_max_date+
    VEG_growing_mean+VEG_growing_min+VEG_growing_max+VEG_growing_95th_prct+VEG_growing_max_95th_prct+
    VEG_growing_AUC_95th_prct+VEG_growing_AUC_v2+
    R_mx_dates+rice_growing_mean+
    rice_growing_min+rice_growing_AUC+rice_growing_95th_prct+
    rice_growing_AUC_v2+rice_growing_AUC_leading+rice_growing_AUC_trailing+I(as.numeric(years))
  
  formula2_dataframe = yield_tn_ha ~plant_dates+harvest_dates+
    VEG_annual_max_95th_prct+VEG_annual_AUC_95th_prct+VEG_growing_max_date+
    VEG_growing_mean+VEG_growing_min+VEG_growing_max+VEG_growing_95th_prct+VEG_growing_max_95th_prct+
    VEG_growing_AUC_95th_prct+VEG_growing_AUC_v2+
    R_mx_dates+rice_growing_mean+
    rice_growing_min+rice_growing_AUC+rice_growing_95th_prct+
    rice_growing_AUC_v2+rice_growing_AUC_leading+rice_growing_AUC_trailing+I(as.numeric(years))+district+years
    
  #distrct boundaries
  districts_lm = readOGR('H:/Projects/India_Index_Insurance/Data/Admin Boundaries','PunjabHaryanaDistricts')
  districts_lm = spTransform(districts_lm, CRS('+proj=sinu +a=6371007.181 +b=6371007.181 +units=m'))
  districts_lm$NAME_2 = toupper(as.character(districts_lm$NAME_2)) 
        
  # Create spatial weights  Remove all districts not in yield_evi
  districts_lm = districts_lm[as.character(districts_lm@data$NAME_2) %in% as.character(unique(yield_evi[,'district'])),]
  districts_polyNB_lm = poly2nb(districts_lm,row.names = row.names(districts_lm)) # polygon continuity$GEOID10
  Wneigh_lm = nb2mat(districts_polyNB_lm, style='W')
  queen = nb2listw(districts_polyNB_lm) 
  # k nearest weights 
  centroids = coordinates(districts_lm)
  knn1 = nb2listw(knn2nb(knearneigh(centroids, k = 1), row.names = districts_lm$NAME_2 ))
  knn2 = nb2listw(knn2nb(knearneigh(centroids, k = 2), row.names = districts_lm$NAME_2 ))
  knn3 = nb2listw(knn2nb(knearneigh(centroids, k = 3), row.names = districts_lm$NAME_2 ))
  knn4 = nb2listw(knn2nb(knearneigh(centroids, k = 4), row.names = districts_lm$NAME_2 ))
  knn5 = nb2listw(knn2nb(knearneigh(centroids, k = 5), row.names = districts_lm$NAME_2 ))
  knn6 = nb2listw(knn2nb(knearneigh(centroids, k = 6), row.names = districts_lm$NAME_2 ))

  OLS = lm(formula2, data=yield_evi)
  OLS_resid = data.frame(model.frame(formula2_dataframe,yield_evi)[,c('years','district')], residuals= OLS$residuals)
  OLS_resid = aggregate(residuals~district,data =OLS_resid, function(x){mean(x,na.rm=T)})
  moran_queen  = moran.test(OLS_resid$residuals,queen)
  moran_knn1  = moran.test(OLS_resid$residuals,knn1)
  moran_knn2  = moran.test(OLS_resid$residuals,knn2)
  moran_knn3  = moran.test(OLS_resid$residuals,knn3)
  moran_knn4  = moran.test(OLS_resid$residuals,knn4)
  moran_knn5  = moran.test(OLS_resid$residuals,knn5)
  moran_knn6  = moran.test(OLS_resid$residuals,knn6)

  Moran_results = data.frame(pvalue = c(moran_queen$p.value,moran_knn1$p.value,moran_knn2$p.value,moran_knn3$p.value,moran_knn4$p.value,moran_knn5$p.value,moran_knn6$p.value),Neighborhood =c('Polygon Continuity',paste('KNN',1:6,sep=' ')), Test_stat =c(moran_queen$statistic[[1]],moran_knn1$statistic[[1]],moran_knn2$statistic[[1]],moran_knn3$statistic[[1]],moran_knn4$statistic[[1]],moran_knn5$statistic[[1]],moran_knn6$statistic[[1]]) )
```  

To avoid overstating the statistical significance of regression coefficients we test the residuals from a pooled linear for spatial autocorrelation [@Fotheringham2002]. Here we estimate equation `r F1=Formula_number` `r paste(F1)`: `r Formula_number=Formula_number+1`

(`r paste(F1)`)                
$\ Y_{it}=\alpha+\sum_{a=1}^{k}\beta_{k}X_{it}+ \mu_{it}$

Where $\ Y_{it}$ is a vector of our dependent variable, district yields in tons per hectare (*yield_tn_ha*), for each district *i* for each year *t*, $\ \alpha$ is an intercept term, $\ \beta_k$ is a vector of *k* coefficients, $\ X_{it}$ corresponds to the *k* independent variables describing NDVI **EVI**, and $\ \mu_{it}$ is pooled error term.

Because a better implementation of the Moran's I for panel regression residuals is not available, tests for spatial autocorrelation are run on the mean regression residuals $\overline{\mu_{i}}$ for each district (*i*)  [@Cliff1981]. Tests were carried out for seven potential neighborhood definitions including six different measures of K-nearest Neighbor, and a queen's polygon continuity (Figure `r paste(Figure_number)`). Moran's I test statistics are less than `r round(mean(Moran_results$Test_stat),2)`, which fails to reject (p <=`r round(min(Moran_results$pvalue),2)`) the null hypothesis of spatial independence. 
  
*Figure `r paste(Figure_number)`: Statistical significance of Moran's I on pooled regression residuals`r c234=Figure_number` `r Figure_number =Figure_number+1 `*
```{r Panel: Spatial autocorrelation plot, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(data=Moran_results,aes(y=pvalue,x=factor(Neighborhood),group=Neighborhood,fill=Neighborhood))+ geom_bar(stat = "identity")+geom_hline(yintercept = 0.05,colour='grey',size=1.5,linetype=5)+ scale_y_continuous(breaks=c(0,0.05,.1,.2,.4,.6))+ theme(legend.position="none",axis.text.x  = element_text(angle=90, vjust=0.5))+labs(x = 'Neighborhood Type', y='P-Value')

``` 

####Tests: Pooled, Fixed or Random  


```{r Panel: Hausman test, echo=FALSE, message=FALSE, warning=FALSE}
# run hausman test
  formula2 = yield_tn_ha ~plant_dates+harvest_dates+
    VEG_annual_max_95th_prct+VEG_annual_AUC_95th_prct+VEG_growing_max_date+
    VEG_growing_mean+VEG_growing_min+VEG_growing_max+VEG_growing_95th_prct+VEG_growing_max_95th_prct+
    VEG_growing_AUC_95th_prct+VEG_growing_AUC_v2+
    R_mx_dates+rice_growing_mean+
    rice_growing_min+rice_growing_AUC+rice_growing_95th_prct+
    rice_growing_AUC_v2+rice_growing_AUC_leading+rice_growing_AUC_trailing+I(as.numeric(years))
  
  formula2_dataframe = yield_tn_ha ~plant_dates+harvest_dates+
    VEG_annual_max_95th_prct+VEG_annual_AUC_95th_prct+VEG_growing_max_date+
    VEG_growing_mean+VEG_growing_min+VEG_growing_max+VEG_growing_95th_prct+VEG_growing_max_95th_prct+
    VEG_growing_AUC_95th_prct+VEG_growing_AUC_v2+
    R_mx_dates+rice_growing_mean+
    rice_growing_min+rice_growing_AUC+rice_growing_95th_prct+         rice_growing_AUC_v2+rice_growing_AUC_leading+rice_growing_AUC_trailing+I(as.numeric(years))+district+years
  
 
  #removed  season_length VEG_annual_sd VEG_growing_AUC_leading+VEG_growing_AUC_trailing++VEG_growing_AUC rice_growing_max VEG_growing_sd rice_growing_AUC_95th_prct rice_growing_max_95th_prct VEG_all_growing_95th_prct
  
  # run fixed and random effects
  fixed <- plm(formula2, data=yield_evi, index=c("district", "years"), model="within")
  random <- plm(formula2, data=yield_evi, index=c("district", "years"), model="random")
  pooled <- plm(formula2, data=yield_evi, index=c("district", "years"), model="pooling")
 # znp <- pvcm(formula2,data=yield_evi,model="within") # not enough obs to run 

 
 # test_pool1=pooltest(znp,fixed)  #F test It is a standard F test, based on the comparison of a model obtained for the full sample and a model based on the estimation of an equation for each individual. null:  test the hypothesis that all the coefficients are equal (pooled OLS better than fixed)  https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf
  
  test_pool2=pooltest(pooled,fixed) # null:  pooled OLS better than fixed
    
  test_pool_random = plmtest(pooled, type=c("bp")) #The null hypothesis in the LM test is that variances across entities is zero (RE inappropriate). This is, no significant difference across units (i.e. no panel effect), ALT; RE better http://www.princeton.edu/~otorres/Panel101R.pdf
  
  test_hausman_fixed_random = phtest(fixed, random) # use fixed if significant To decide between fixed or random effects you can run a Hausman test where the null hypothesis is that the preferred model is random effects vs. the alternative the fixed effects (see Green, 2008, chapter 9). It basically tests whether the unique errors (ui ) are correlated with the regressors, the null hypothesis is they are not. http://www.indiana.edu/~wim/docs/10_7_2011_slides.pdf
  #pander(hausman)
```

Testing is required to choose the proper estimation method for panel regression. We must choose between pooled, fixed effect (FE), and random effect (RE) models. First we can test for poolability of our model. Pooled regression assumes a constant intercept and slopes between different districts and time periods. We can test if variance across districts is equal to zero using an F-test. Here we reject the use of pooled OLS (p <= `r test_pool2$p.value`) in favor of a fixed effects model with unique intercepts for each district. We can then compare the use of FE and RE models. The hausman test checks for exogeniety of the unobserved error component, tf the null hypothesis is rejected, the RE model is inconsistent, and the FE model will be preferred. If effects are exogenous both fixed (FE) and random effects are asymptotically equivalent. Here we test if $\ H_{0}:\hat{\beta}_{RE}=\hat{\beta}_{RE}$, where $\hat{\beta}_{RE}$ are coefficient vectors of time-varying explanitory variables. Here we cannot reject the null hypothesis  (p >= `r substr(test_hausman_fixed_random$p.value,1,4) `) and choose to use the random effects estimator as it will be more efficient.  

 
####Multicolinearity & Principal Components Analysis Transform

Multicolinearity, high correlations between independent variables, can increase estimates of a variable's estimated variance. This can have the adverse effect of creating models in which the $\ R^2$ is high and no variables are statistically significant. Multicolinearity can also produce coefficients of the 'wrong sign' and of unreasonable magnitude [@o2007caution; @greeneh]. Here we use a variance inflation factor (VIF) to quantify how much the variance is inflated for each coefficient[^1]. VIF values over 4 for any variable are generally considered problematic and require further examination [@greeneh]. Here we present summary statistics for VIFs on a ordinary least squares estimation of all model variable in Table (`r paste(Table_number)`) below:

[^1]: A nice synopsis of VIF can be found here [@State2016].

*Table `r paste(Table_number)`: Variance inflation factor summary table `r Table_number =Table_number+1 `*

```{r,echo=F}
#https://onlinecourses.science.psu.edu/stat501/node/347
fit =lm(formula2,data=yield_evi)
stargazer(as.data.frame(vif(fit)),type = 'text')
```


```{r Panel:  PCA estimation , echo=FALSE}
  # include these above estimation 
  # formulas
  formula_PCA =yield_tn_ha~lag1_PC1+ rcs(PC1,4)+rcs(PC2,4)+rcs(PC3,4)+rcs(PC4,4)+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14+PC15+PC16+PC17+PC18+PC19 +PC20+PC21
  formula_PCA_dataframe = yield_tn_ha~lag1_PC1+ rcs(PC1,4)+rcs(PC2,4)+rcs(PC3,4)+rcs(PC4,4)+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14+PC15+PC16+PC17+PC18+PC19+PC20+PC21 +district+years   
  ###########################
  # USE PCA 
  pca_input = na.omit(model.frame(formula2_dataframe,yield_evi))
  pca_data = pca_input[,sapply(pca_input,is.numeric)] # limit to numeric number data
  pca_data = pca_data[,!(names(pca_data) %in% c('area','production_tonnes','yield_tn_ha'))] # remove dependent variable data
  pca = prcomp( pca_data, scale = T,center = T ) 
 
  pca_pred = as.data.frame(stats::predict(pca))
  pca_pred$district = pca_input$district
  pca_pred$years = pca_input$years
  pca_pred$yield_tn_ha =pca_input$yield_tn_ha
  
  # IMPORTANT: ORDER TO AVOID PROBLEMS WITH INDEX LATER 
  pca_pred=pca_pred[with(pca_pred, order(district, years)), ]
```
To avoid problems with multicolinearity between independent variables we apply a principal components analysis transformation (PCA) to a centered and scaled matrix of independent variables $\ X$. PCA allows for the replacement of $\ X$ with a new matrix whos variables are orthoganal to each other but span the multidimensional space of $\ X$ [@GELADI19861]. In this case we generate `r paste(dim(pca$rotation)[1])` principal components for inclusion in a random effects panel regression. A table of PCA component imporance can be found in the appendix, Table(A`r AT1=Appendix_Table_number``r paste(AT1)``r Appendix_Table_number=Appendix_Table_number+1`).
 
 
 
 
 
####OLS Regression Estimation

```{r LM for prediction, message=FALSE, warning=FALSE, include=FALSE}
# formulas
  formula_lm =yield_tn_ha~ rcs(PC1,4)+rcs(PC2,4)+rcs(PC3,4)+rcs(PC4,4)+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14+PC15+PC16+PC17+PC18+PC19 +PC20+PC21
  formula_lm_dataframe = yield_tn_ha~ rcs(PC1,4)+rcs(PC2,4)+rcs(PC3,4)+rcs(PC4,4)+PC5+PC6+PC7+ PC8+PC9+PC10+PC11+PC12+PC13+PC14+PC15+PC16+PC17+PC18+PC19+PC20+PC21 +district+years   
 
    # estimate plm
  lm1 <- lm(formula_lm, data=pca_pred)
 
  #stargazer(random_pca, type="text")
  summary(lm1)
   
  # predict lm to all dates
  lm_pred = predict(lm1,newdata = pca_pred,se.fit=T)
  lm_pred

  # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709
  SSR_FULL = sum((pca_pred$yield_tn_ha-lm_pred$fit)^2)
  SSR_FE = sum( lm(yield_tn_ha ~ 1 +as.factor(district) , data=pca_input)$residuals^2)
  Witin_R2 =  1 - (SSR_FULL/SSR_FE)  
  print(paste('Within R2',round(Witin_R2,2)))

   # get prediction and and model.frame 
  model_data_lm = data.frame(pca_pred,LM_Fit = lm_pred$fit)
  model_data_lm$district = as.character(model_data_lm$district)
  model_data_lm$years_id = as.numeric(substr(model_data_lm$year,1,4))
  model_data_lm = model_data_lm[,c('district','years_id','yield_tn_ha','LM_Fit')]
  model_data_lm = melt(model_data_lm,id = c('years_id','district'))
  #windows()
  ggplot(data=model_data_lm,aes(x=as.factor(years_id),y=value,colour=variable,alpha=0.5))+
    geom_point(size=2) + facet_wrap( ~ district )+xlab('Year')+ylab('Wheat Tons / ha')+ theme(legend.position="none")+ 
    theme(axis.text.x  = element_text(angle=90, vjust=0.5))
  
  
```  



####Panel Regression Estimation

We use panel data to model wheat yields over time at the district level. The use of **(spatial)** panel data in this study helps to alleviate two key problems, unobserved spatial and temporal dynamics, and homogeneity (lack of variance). When pooled together, the integration of two statewide data sets (N = `r paste(length(unique(model.frame(formula2_dataframe,yield_evi)[,'district'])))`) over the `r paste(range(yield_ndvi$years, na.rm=T),collapse='-')` period (T = `r paste(length(unique(model.frame(formula2_dataframe,yield_evi)[,'years'])))`) provides `r  length(unique(model.frame(formula2_dataframe,yield_evi)[,'years']))*length(unique(model.frame(formula2_dataframe,yield_evi)[,'district']))` observations. Compared to cross-sectional approaches this amounts to a substantial increase in the degree of observed variance over both space and time.

Here we estimate equation `r F1=Formula_number` `r paste(F1)`: `r Formula_number=Formula_number+1`

(`r paste(F1)`)                
$\ Y_{it}=\alpha+\beta_{1}PC(1)_{it-1}+\sum_{k=2}^{H}\beta_{k}PC(k)_{it}+ \mu_{it} +\epsilon_{it}$
 

Where $\ Y_{it}$ is a vector of our dependent variable, district yields in tons per hectare (*yield_tn_ha*), for each district *i* for each year *t*, $\ \alpha$ is an intercept term, $\ \beta_{1}PC(1)_{it-1}$ is a one-period lag of the first pricipal component, $\ \beta_k$ is a vector of *k* coefficients, $\ X_{it}$ corresponding to the *H* principal components, $\ \mu_{it}$ is the between entity error term, and $\ \epsilon_{it}$ is the within entity error term. To capture non-linear effects here we apply natural cubic splines to the first `r english(length(grep('rcs', strsplit(as.character(formula_PCA)[3],'+',fixed = T)[[1]])))` principal components [@devlin1986spline] [^2]. Natural cubic splines use cubic terms in the center of the data and restrict the ends to straight lines, preventing distortion at extreme values. When reported splines are noted with rcs(variable_name, number_of_knots) with order noted by ' and ''.

[^2]: The first four principal components in this PCA comprises `r paste(round(summary(pca)$importance[3,4]*100),2)` % of total variance in $\mathbf{X}$


###Spatial Panel Regression


####Defining the neighborhood


#Results
##Focus Group Interviews
 



##Panel Regression

Estimates of equation (`r paste(F1)`) are provide below:

*District level random effects estimation of wheat yields in tons per hectare*
```{r Panel: Fitted vs actual PCA estimate, echo=FALSE}

  # add a time lag  or dif
  pca_pred <- pdata.frame(pca_pred, index = c("district", "years"))
  pca_pred$lag1_yield_tn_ha = lag(pca_pred$yield_tn_ha,1)
  pca_pred$diff1_yield_tn_ha = diff(pca_pred$yield_tn_ha,1)
  pca_pred$diff1_PC1 = diff(pca_pred$PC1,1)
  pca_pred$lag1_PC1 = lag(pca_pred$PC1,1)

  # estimate plm
  random_pca <- plm(formula_PCA, data=pca_pred, index=c("district", "years"), model="random")
 
  #stargazer(random_pca, type="text")
  summary(random_pca)
   
  # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709
  SSR_FULL = sum(random_pca$residuals^2)
  SSR_FE = sum( plm(yield_tn_ha ~ 1 +as.factor(district) , data=pca_input, index=c("district", "years"), model="pooling")$residuals^2)
  Witin_R2 =  1 - (SSR_FULL/SSR_FE)  
  print(paste('Within R2',round(Witin_R2,2)))
  
```


*Figure `r paste(Figure_number)`: Fitted vs actual for estimation of equation (`r paste(Formula_number)`)  `r aaa=Figure_number` `r Figure_number =Figure_number+1 `*
```{r Panel: Fitted vs actual PCA PLOT, fig.width=10, fig.height=6, echo=FALSE}
  # get prediction and and model.frame 
  fitted_pca = data.frame(PCA_Fit = random_pca$model[[1]] - random_pca$residuals)
  model_data_pca = cbind(as.data.frame(as.matrix(random_pca$model)),fitted_pca)
  model_data_pca = cbind(model_data_pca,na.omit(model.frame(formula_PCA_dataframe,pca_pred)))  
  model_data_pca$district = as.character(model_data_pca$district)
  model_data_pca$years_id = as.numeric(substr(model_data_pca$year,1,4))
  model_data_pca = model_data_pca[,c('district','years_id','yield_tn_ha','PCA_Fit')]
  model_data_pca = melt(model_data_pca,id = c('years_id','district'))
  
  ggplot(data=model_data_pca,aes(x=as.factor(years_id),y=value,colour=variable,alpha=0.5))+
    geom_point(size=2) + facet_wrap( ~ district )+xlab('Year')+ylab('Wheat Tons / ha')+ theme(legend.position="none")+ 
    theme(axis.text.x  = element_text(angle=90, vjust=0.5))
  
# plots of model performance 
#ggplot(data=melt(data.frame(Residuals = random_pca$residuals,Yield_tn_ha=random_pca$model[[1]],Prc_Error=random_pca$residuals/random_pca$model[[1]])),aes(x=value,fill=variable))+geom_histogram(bins=50)

ggplot(data= data.frame(Prc_Error=random_pca$residuals/random_pca$model[[1]]*100),aes(x=Prc_Error))+geom_histogram(bins=30)+xlab('% Error')

  error_pca = data.frame(PCA_Error = random_pca$residuals)
  error_data_pca = cbind(error_pca,na.omit(model.frame(formula_PCA_dataframe,pca_pred)))  
  error_data_pca$Per_Error =  (error_data_pca$PCA_Error/ error_data_pca$yield_tn_ha)*100
  error_data_pca$years_id = as.numeric(substr(error_data_pca$year,1,4))
  error_data_pca$district = as.character(error_data_pca$district)
  error_data_pca = error_data_pca[,c('district','years_id','Per_Error')]
  error_data_pca = melt(error_data_pca,id = c('years_id','district'))

 
# histogram of errors stacked histogram use in presentation
  # library(plotly)
  # ggplot(data=error_data_pca,aes(x=value,fill=district))+ geom_histogram( )+xlab('% Error')+ theme(legend.position="none")
  # ggplotly()
  # 
  # 
  # ggplot(data= data.frame(Prc_Error=abs(random_pca$residuals/random_pca$model[[1]]*100)),aes(x=Prc_Error)) +   stat_ecdf(geom = "step",size=1)+xlab('Absolute value of % error')+ylab('Cumulative Density')
  # 
  # ggplot(data= data.frame(Prc_Error=abs(random_pca$residuals/random_pca$model[[1]]*100)),aes(x=Prc_Error)) + stat_ecdf(geom = "step",size=1)+xlab('Absolute value of % error')+ylab('Cumulative Density')

```


##Spatial Panel Regression

  *Actual values (red), predicted values (blue)*
```{r Panel: Spatial Panel Estimation, fig.width=10, fig.height=6, echo=FALSE}
  #yeild and evi data
  pca_pred_splm = as.data.frame(pca_pred)

  #distrct boundaries
  districts_plm = readOGR('H:/Projects/India_Index_Insurance/Data/Admin Boundaries','PunjabHaryanaDistricts')
  districts_plm = spTransform(districts_plm, CRS('+proj=sinu +a=6371007.181 +b=6371007.181 +units=m'))
  districts_plm$NAME_2 = toupper(as.character(districts_plm$NAME_2)) 
  
 # get District outlines
  districts_plm = readOGR('H:/Projects/India_Index_Insurance/Data/Admin Boundaries','PunjabHaryanaDistricts')
  districts_plm = spTransform(districts_plm, CRS('+proj=sinu +a=6371007.181 +b=6371007.181 +units=m'))
  districts_plm$NAME_2 = toupper(as.character(districts_plm$NAME_2)) 
  
 # find all districts with 10 years
  balanced_panel = as.character(as.data.frame(table(pca_pred_splm$district))$Var1[as.data.frame(table(pca_pred_splm$district))$Freq ==10])
  
  # confirm balanced 
  table(pca_pred$years[pca_pred$district %in% balanced_panel])
  pca_pred_balanced = pca_pred_splm[pca_pred_splm$district %in% balanced_panel,]
                                                
  # IMPORTANT: ORDER TO AVOID PROBLEMS WITH INDEX LATER 
  pca_pred_balanced=pca_pred_balanced[with(pca_pred_balanced, order(district, years)), ]                                            
       
        
  # Create spatial weights  Remove all districts not in balanced panel
  districts_plm = districts_plm[as.character(districts_plm@data$NAME_2) %in% as.character(unique(pca_pred_balanced[,'district'])),]
  districts_polyNB = poly2nb(districts_plm,row.names = row.names(districts_plm)) # polygon continuity$GEOID10
  Wneigh = nb2mat(districts_polyNB, style='W')
  districts_polyListw = nb2listw(districts_polyNB) 
  
  
 # SPLM - Spatial lag estimation  -------------------------------------------------
  formula_PCA_splag2 = yield_tn_ha~ rcs(PC1,4)+rcs(PC2,4)+rcs(PC3,4)+rcs(PC4,4)+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14+PC15+PC16+PC17+PC18+PC19 +PC20+PC21
  formula_PCA_splag2_dataframe = yield_tn_ha~rcs(PC1,4)+rcs(PC2,4)+rcs(PC3,4)+rcs(PC4,4)+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14+PC15+PC16+PC17+PC18+PC19+PC20+PC21+district+years
  
  sararremod <- spml(formula_PCA_splag2, data = pca_pred_balanced, index = c("district", "years"),
                     listw = districts_polyListw, model = "random", lag = TRUE, spatial.error = "b")
  summary(sararremod)
  summary(sararremod)$rsqr
  
  # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709
  SSR_FULL = sum(sararremod$residuals^2)
  SSR_FE = sum( spml(yield_tn_ha~1 +as.factor(district), data = pca_pred_balanced, index = c("district", "years"),
                     listw = districts_polyListw, model = "random", lag = F, spatial.error = "none")$residuals^2)
  Witin_R2 =  1 - (SSR_FULL/SSR_FE)    
  Witin_R2  
  
  # plot spatial lag panel regression 
  model_pca_sararremod = data.frame(SPLM_Fit = sararremod$model[[1]] - sararremod$residuals)
  model_pca_sararremod = cbind(as.data.frame(as.matrix(sararremod$model)),model_pca_sararremod)
  model_pca_sararremod = cbind(model_pca_sararremod,na.omit(model.frame(formula_PCA_splag2_dataframe,pca_pred_balanced)))  
  model_pca_sararremod$district = as.character(model_pca_sararremod$district)
  model_pca_sararremod$years_id = as.numeric(substr(model_pca_sararremod$year,1,4))
  model_pca_sararremod = model_pca_sararremod[,c('district','years_id','yield_tn_ha','SPLM_Fit')]
  model_pca_sararremod = melt(model_pca_sararremod,id = c('years_id','district'))
  model_pca_sararremod$variable= as.character(model_pca_sararremod$variable)
  model_pca_sararremod$variable[model_pca_sararremod$variable=='fitted']='Fitted Spatial'
  
   ggplot(data=model_pca_sararremod,aes(x=as.factor(years_id),y=value,colour=variable,alpha=0.5))+
    geom_point(size=2) + facet_wrap( ~ district )+xlab('Year')+ylab('Wheat Tons / ha')+ theme(legend.position="none")+ 
    theme(axis.text.x  = element_text(angle=90, vjust=0.5))
   
```


```{r Panel: OLS VS TEMPORAL PANEL VS SPATIAL PANEL, fig.width=10, fig.height=6, echo=FALSE}

# Plot temporal and spatial fit  ------------------------------------------
  
  space_time_fit = rbind(model_data_pca,model_pca_sararremod,model_data_lm)
  
  space_time_fit = space_time_fit[space_time_fit$district %in% unique(model_pca_sararremod$district),]
  space_time_fit = space_time_fit[space_time_fit$district %in% unique(model_data_lm$district),]

  levels(space_time_fit$variable) =c('Actual Yields', 'PCA Fit','SPLM Fit','OLS Fit')
  #windows()
  # ggplot(data=space_time_fit,aes(x=as.factor(years_id),y=value,colour=variable,alpha=0.2))+
  #   geom_point(size=2)  + scale_colour_manual(values = c("black",'purple','green', "mediumblue")) + facet_wrap( ~ district,scales = 'free' )+xlab('Year')+ylab('Wheat Tons / ha')+  
  #   theme(axis.text.x  = element_text(angle=90, vjust=0.5))
  
    ggplot()+geom_line(data=space_time_fit[space_time_fit$variable=='Actual Yields',],aes(x=years_id,y=value,colour=variable),size=1.2)+  
    geom_point(data=space_time_fit[space_time_fit$variable!='Actual Yields',],aes(x= years_id,y=value,colour=variable),alpha=0.6,size=2)+ facet_wrap( ~ district,scales = 'free' )+xlab('Year')+ylab('Wheat Tons / ha')+  
    theme(axis.text.x  = element_text(angle=90, vjust=0.5))+ scale_x_continuous(breaks=c(2005,2010, 2015))
  
```  
  

#Discussion


#Conclusions


 

```{r pressure, echo=FALSE}
# plot(pressure)
# library(stargazer)
# stargazer(yield_ndvi[,8:10] ,  title="Descriptive statistics", digits=2,summary = T,font.size='small')

```
 

#Appendix A
##Yield Data
*Table A`r paste(Appendix_Table_number)`: Rabi Season Wheat Yields Metric Tons per Hectare by State `r Appendix_Table_number =Appendix_Table_number+1 `*
```{r, include=T,echo=F,warning=F}
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', F)
state_yeilds = ddply(yield_ndvi,~state+district,summarise,Min=min(yield_tn_ha,na.rm=T),Mean=mean(yield_tn_ha,na.rm=T),Max=max(yield_tn_ha,na.rm=T))
names(state_yeilds)[1]="State"
pander(state_yeilds, justify = c('left', 'left', 'center','center', 'center'))
```

##Principal Components Analysis
*Table A`r paste(AT1)`: Importance of PCA components*

```{r Panel:  PCA output  , echo=FALSE}
   summary(pca) 
```



##Functions
*Functions `r paste(A)`: Planting/Harvest date functions*
```{r, include=T,echo=T,warning=F}
PlantHarvestDates = function(start_date,end_date,PlantingMonth,PlantingDay,HarvestMonth,HarvestDay){
    # this function takes in date range and returns planting and harvest date for time series as a data.frame 
    # for all years of interest. Handles growing periods overlaping a new year properly.
    # NOTE: This is used to create dataframe of planting / harvest dates for many other functions
    #  
    # e.g. PlantHarvest = PlantHarvestDates('2002-01-01','2016-02-02',PlantingMonth=11, PlantingDay=23,HarvestMonth=4,HarvestDay=30)
     
    start_end_years = c(strptime(start_date,'%Y-%m-%d'),strptime(end_date,'%Y-%m-%d'))
    names(unclass(start_end_years[1]))
    start_end_years[1]$mon=PlantingMonth-1
    start_end_years[1]$mday=PlantingDay
    planting = as.Date(seq(start_end_years[1],
      length=strptime(dates[2],'%Y-%m-%d')$year-strptime(dates[1],'%Y-%m-%d')$year,
      by='year'))
    # set harvest
    start_end_years[2]$year=start_end_years[1]$year+1    # set year equal to start year +1
    start_end_years[2]$mon=HarvestMonth-1
    start_end_years[2]$mday=HarvestDay
    harvest = as.Date(seq(start_end_years[2],
      length=strptime(end_date,'%Y-%m-%d')$year-strptime(start_date,'%Y-%m-%d')$year,
      by='year'))
    return(data.frame(planting=planting,harvest=harvest))
  }

SearchMinumumBeforeAfterDOY = function(x,dates_in,DOY_in,days_shift,dir){
  	# calculates the global minimum for days before,after,both of expected planting date
    # best to set DOY as the last expected date of planting
    # x = vegetation index, dates_in = dates of observation POSIX, DOY_in = expected planting or harvest date
    # days_shift = # days to search around DOY_in,  dir='before' 'after' 'beforeafter'
  
  	if(days_shift<=8){print('Using less than 8 days is dangerous, 15-30 stable')}
  
  	# avoid problems with time class
  	if(is.na(DOY_in[1])){print('ERROR: convert date format to %Y%j');break}
  	if(class(dates_in)[1]!= 'POSIXlt' ){dates_in=as.POSIXlt(dates_in)}
  
  	# limit to fixed # of days before/after DOY
      DOY_in = as.POSIXlt(DOY_in)
      DOY_before = DOY_in
  
  	#names(unclass(DOY_before[1]))
  	if(dir=='before') DOY_before$mday=DOY_before$mday-days_shift      # set days before to doy - days_before
  	if(dir=='after') DOY_before$mday=DOY_before$mday+days_shift      # set days before to doy - days_before
      if(dir=='beforeafter'){ DOY_before$mday=DOY_before$mday-days_shift 
        DOY_in$mday=DOY_in$mday+days_shift}
  	DOY_table = data.frame(DOY_before=DOY_before,DOY_in=DOY_in)   #join start end search dates
  
    # list all days 'days_before' DOY_in
     if(dir=='before'|dir=='beforeafter'){ DOY_interest = as.POSIXlt(unlist(lapply(1:dim(DOY_table)[1],
  	  function(h){format(seq(DOY_table[h,1],
                DOY_table[h,2],by='day'),'%Y-%m-%d')})),tz='UTC')}
    if(dir=='after'){DOY_interest = as.POSIXlt(unlist(lapply(1:dim(DOY_table)[1],
  	  function(h){format(seq(DOY_table[h,2],
                DOY_table[h,1],by='day'),'%Y-%m-%d')})),tz='UTC')}
  
    # find all local minima, and match with DOY
    x_DOY_interest = x[dates_in %in% DOY_interest]
    dates_DOY_interest = dates_in[dates_in %in% DOY_interest]
    # get min value for this period for each year
    sort(AnnualMaxima(x_DOY_interest*-1,as.Date(dates_DOY_interest)))
}
```

*Functions `r paste(B)`: Flexible growing season vegetation metrics*
```{r, include=T,echo=T,warning=F}
PeriodAggregator = function(x,dates_in,date_range_st, date_range_end,by_in='days',FUN){
    	# returns a summary statistic of x for any function FUN, over the period defined by date_range_st, date_range_end
      # x = vegetation index data, dates_in = dates of observation POSIX, dates_in,date_range_st = start end dates of period, FUN = function
      # E.g. PeriodAggregator(x=plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates,date_range_st=plotdatasmoothed$dates[1],date_range_end=plotdatasmoothed$dates[20], FUN = function(y){mean(y,na.rm=T)})
    	if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)
    	if(class(date_range_st)[1]== "POSIXct" ){date_range_st = as.Date(date_range_st)
                                             date_range_end = as.Date(date_range_end)}
    	#Avoid problems with missing plant or harvest dates
    	if(length(date_range_st)!=length(date_range_end)){print('number of elements in start end dates dont match');	break}
    	dataout=lapply(1:length(date_range_st),function(z){
      		DateRange = seq(date_range_st[z],date_range_end[z],by=by_in)
      		x=x[dates_in %in% DateRange]
      		dates_in=dates_in[dates_in %in% DateRange]
      		FUN(x)})
    	dataout = do.call(c,dataout)
      names(dataout)=format(date_range_st,'%Y')
  	  dataout
  }
```

*Functions `r paste(C)`: Area under the curve estimation - smoothing splines*
```{r, include=T,echo=T,warning=F}
PeriodAUC = function(x_in,dates_in,DOY_start_in,DOY_end_in){
         # calculate area under the curve by period of the year using spline estimation
         # x = data, dates_in=asDate(dates),DOY_start_in=asDate(list of start periods),DOY_end_in=asDate(list of end per
         # x = plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates , DOY_start_in= plant_dates ,DOY_end_in=harvest_dates)
        if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)
         dates_group = rep(0,length(dates_in))    # create storage for factors of periods
         # get sequences of periods of inerest
         seq_interest = lapply(1:length(DOY_start_in),function(z){seq(DOY_start_in[z],DOY_end_in[z],by='days')})
         # switch dates-group to period group
         years_avail = sort(as.numeric(unique(unlist(
                lapply(seq_interest,function(z) format(z,'%Y'))))))
         for(z in 1:length(seq_interest)){        #assigns year for beginging of planting season
		            dates_group[dates_in %in% seq_interest[[z]]]=years_avail[z]
                assign('dates_group',dates_group,envir = .GlobalEnv) }  # assign doesn't work in lapply using for loop instead
	      # calculate AUC for periods of interest
         FUN = function(q,w){auc(q,w,type='spline')}
         datesY = format(dates_in,'%Y')
         data.split = split(x_in,dates_group)
         d = do.call(c,lapply(2:length(data.split),function(z){   # start at 2 to avoid group=0
	          	FUN(q=1:length(data.split[[z]]),w=data.split[[z]]) }))
         names(d) = names(data.split)[2:length(data.split)]
         d
	}
```
*Functions `r paste(D)`: Area under the curve estimation - trapazoidal estimation*
```{r, include=T,echo=T,warning=F}
PeriodAUC_method2 = function(x_in,dates_in,DOY_start_in,DOY_end_in){
	        #NOTE SPLINE METHOD 1 SEEMS to WORK BETTER
         # calculate area under the curve by period of the year
         # x = data, dates_in=asDate(dates),DOY_start=asDate(list of start periods),DOY_end=asDate(list of end per$
         # x = plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates , DOY_start=annualMinumumBeforeDOY(x = plotd$
         if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)

         dates_group = rep(0,length(dates_in))    # create storage for factors of periods
         # get sequences of periods of inerest
         seq_interest = lapply(1:length(DOY_start_in),function(z){seq(DOY_start_in[z],DOY_end_in[z],by='days')})
         # switch dates-group to period group
         years_avail = sort(as.numeric(unique(unlist(
                lapply(seq_interest,function(z) format(z,'%Y'))))))
         for(z in 1:length(seq_interest)){        #assigns year for beginging of planting season
                dates_group[dates_in %in% seq_interest[[z]]]=years_avail[z]
                assign('dates_group',dates_group,envir = .GlobalEnv) }  # assign doesn't work in lapply using for loop instead
 
        # calculate AUC for periods of interest
         FUN = function(q,w){  sum(diff(q)*rollmean(w,2))}
         datesY = format(dates_in,'%Y')
         data.split = split(x_in,dates_group)
         d = do.call(c,lapply(2:length(data.split),function(z){   # start at 2 to avoid group=0
                FUN(q=1:length(data.split[[z]]),w=data.split[[z]]) }))
         names(d) = names(data.split)[2:length(data.split)]
         #print(cbind(names(data.split)[2:length(data.split)], d))
         d
        }
```

*Functions `r paste(E)`: Base function used for estimating sample quantiles*
```{r, include=T,echo=F,warning=F}
quantile_type8 = function(x){
  quantile(x ,p=Quant_percentile,type=8,na.rm=T)
}
```

*Functions `r paste(F)`: Function to return date of any given phenomenon*
```{r, include=T,echo=F,warning=F}
PeriodAggregatorDates = function(x,dates_in,date_range_st, date_range_end,by_in='days',FUN){
        # returns a date of summary statistic defined by FUN
        # like the date of the maximum value of x for the period defined by date_range_st, date_range_end
        # other parameters identical to other functions show above
        if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)
        if(class(date_range_st)[1]== "POSIXct" ){date_range_st = as.Date(date_range_st)
                                             date_range_end = as.Date(date_range_end)}
        #Avoid problems with missing plant or harvest dates
        if(length(date_range_st)!=length(date_range_end)){print('number of elements in start end dates dont match');break}

        dataout=lapply(1:length(date_range_st),function(z){
            DateRange2 = seq(date_range_st[z],date_range_end[z],by=by_in)
            x2 = x[dates_in %in% DateRange2]
            dates_in2 = dates_in[dates_in %in% DateRange2]
            which_max = which(FUN(x2) ==  x2)
         		if(length(which_max)>1){
          		    which_max = c(which_max[1],which_max[length(which_max)]) # limit to only 2 
          			if((which_max[2]-which_max[1])==1){
          				which_max=which_max[1]  # favor the first instance of maximum
          			  } else if((which_max[2]-which_max[1])==2){
          				which_max=which_max[1]+1 # is seperated by 2 choose middle left
                  } else if((which_max[2]-which_max[1])==3){
          				which_max=which_max[1]+2} # is seperated by 3 choose middle
        		}
            max_dates = dates_in2[which_max]
                })
        dataout = do.call(c,dataout)
        names(dataout)=format(date_range_st,'%Y')
        dataout
    }
```

*Functions `r paste(G)`: Mean day of the year values*
```{r, include=T,echo=F,warning=F}
  AnnualAverageDOYvalues = function(x,dates_in){
    	# calculates the average value for DOY for the whole series
    	datesj = format(dates_in,'%j')
    	do.call(c,lapply(split(x,datesj),function(y){mean(y,na.rm=T)}))}
```

*Functions `r paste(H)`: Smoothing splines with outlier removal*
```{r, include=T,echo=F,warning=F}
#---------------------------------------------------------------------
# This function takes a time series w/ dates (x, dates) and returns a spline smoothed time series with outliers removed.
# Outliers are identified as points with absolute value more than out_sigma * sd, where sd is the residual
# standard deviation between the input data and the initial spline fit, and out_sigma is a variable
# coefficient. The spline smoothing parameter spline_spar controls the smoothness of the fit (see spline.smooth help)
# and out_iterations controls the number of times that outliers are checked and removed w/ subsequent spline refit
# pred_dates is a vector of dates where spline smoothed predictions of x are desired. If NA, then a daily series spanning
# min(dates)-max(dates) is returned
SplineAndOutlierRemoval <- function(x, dates, out_sigma=3, spline_spar=0.3, out_iterations=1,pred_dates){
  dates <- as.numeric(dates) # spline doesn't work with dates
  pred_dates = as.numeric(pred_dates)
  # if prediction dates aren't provided, we assume we want daily ones
  if(is.na(pred_dates[1])){
    pred_dates <- min(dates, na.rm=T):max(dates, na.rm=T)}
  # eliminate outliers and respline
  for(i in 1:out_iterations){
    # fit a smoothing spline to non-missing data
    spl <- try(smooth.spline(dates[!is.na(x)], x[!is.na(x)], spar=spline_spar), silent=T)
    if(inherits(spl, 'try-error')){
      print("Failed to fit smoothing spline")
      return(NA)
    }
    smooth_x <- try(predict(spl, dates)$y, silent=T) # calculate spline smoothed values
    if(inherits(smooth_x, 'try-error')){
      print("Failed to predict with spline")
      return(NA)
    }
    smooth_x_resid <- x - smooth_x # calculate residuals from spline
    smooth_x_resid_sd <- try(sd(smooth_x_resid, na.rm=T), silent=T) # standard dev of absolute value of residuals
    if(inherits(smooth_x_resid_sd, 'try-error')){
      print("Failed to get sd of residuals")
      return(NA)
    }
    outliers <- abs(smooth_x_resid) > out_sigma * smooth_x_resid_sd
    outliers[is.na(outliers)] <- F
    if(sum(outliers) > 0){
      # if we found outliers, eliminate them in x and refit up to iterations
      x[outliers] <- NA
    }else{
      # if we didn't find any outliers, we abandon the iteration and return the smoothed values
      smooth_x_return <- try(predict(spl, pred_dates)$y, silent=T)
      if(inherits(smooth_x_return, 'try-error')){
        print("No outliers, but failed to predict with final spline")
        return(NA)
      }else{
        return(smooth_x_return)
      }
    }
  }
  # fit the spline to the outlier screened data, then return the predicted series
  spl <- try(smooth.spline(dates[!is.na(x)], x[!is.na(x)], spar=spline_spar), silent=T)
  if(inherits(spl, 'try-error')){
    print("Failed to predict with final spline")
    return(NA)
  }else{
    smooth_x_return <- try(predict(spl, pred_dates)$y, silent=T)
    if(inherits(smooth_x_return, 'try-error')){
      return(NA)
    }else{
      return(smooth_x_return)
    }
  }
}
```

*Function `r paste(I)`: Flexible annual vegetation metrics*
```{r, include=T,echo=T,warning=F}
  AnnualAggregator = function(x,dates_in,FUN){
    # returns an annual summary statistic of any function
    # x = vegetation index data, dates_in = dates of observation POSIX,
    # E.g. AnnualAggregator(x=  plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates, FUN = function(y){mean(y,na.rm=T)})
    datesY = format(dates_in,'%Y')
    do.call(c,lapply(split(x,datesY),FUN))}
```

*Function `r paste(J)`: Rapid multicore extract raster data by point or polygon*
```{r, include=T,echo=T,warning=F}
extract_value_point_polygon = function(point_or_polygon, raster_stack, num_workers){
          # Returns list containing values from locations of spatial points or polygons
 	  # if polygons are too small reverts to centroid 
          if(class(raster_stack)!='list'){raster_stack=list(raster_stack)}
          lapply(c('raster','foreach','doParallel'), require, character.only = T)
          registerDoParallel(num_workers)
          ptm <- proc.time()
          # iterate between points or polygons
          ply_result = foreach(j = 1:length(point_or_polygon),.inorder=T) %do%{
                print(paste('Working on feature: ',j,' out of ',length(point_or_polygon)))
                get_class= class(point_or_polygon)[1]
                # switch rasterstack according to which point or polygon is %over%
                for(z in 1:length(raster_stack)){
                        # set raster to use
                        raster_stack_use = raster_stack[[z]]
                        # get cell numbers of point of polygon, repeat if missing
                        if(get_class=='SpatialPolygons'|get_class=='SpatialPolygonsDataFrame'){
                            cell = as.numeric(cellFromPolygon(raster_stack_use, point_or_polygon[j,], weights=F)[[1]])
                            # if polygon is too small to find cells, convert to centroid and get cellfromXY
                           if(length(cell)==0){                                       #coord(poly) returns centroid
                                cell = as.numeric(na.omit(cellFromXY(raster_stack_use, coordinates(point_or_polygon[j,]) )))}}
                        if(get_class=='SpatialPointsDataFrame'|get_class=='SpatialPoints'){
                            cell = as.numeric(na.omit(cellFromXY(raster_stack_use, point_or_polygon[j,])))}
                        # if cells found keep raster_stack_use = raster_stack[[z]]
                        if(length(cell)!=0){break}
                        # if cells not found repeat for different stack or return NA
                        if(length(cell)==0 & z!=length(raster_stack)){next}else{return(NA)}
                }
                # create raster mask from cell numbers
                r = rasterFromCells(raster_stack_use, cell,values=F)
                result = foreach(i = 1:dim(raster_stack_use)[3],.packages='raster',.inorder=T) %dopar% {
                   crop(raster_stack_use[[i]],r)
                }
                result=as.data.frame(getValues(stack(result)))
                return(result)
          }
          print( proc.time() - ptm)
          endCluster()
          return(ply_result)
 }
```


References {#references .unnumbered}
==========


