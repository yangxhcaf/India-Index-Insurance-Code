---
title: "Modeling Wheat Yields in India: Describing & Exploiting Spatio-Temporal Variability with Panel Regression"
bibliography: MyCollection.bib
output: word_document
---

```{r setup, include=FALSE}
# citation styles can be managed here: http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
# http://rmarkdown.rstudio.com
# http://rmarkdown.rstudio.com/authoring_pandoc_markdown.html#headers
  rm(list=ls())
  setwd('H:/Projects/India_Index_Insurance/India_Index_Insurance_Code/WriteUp/')
  source('H:/Scripts/multi_grep_character.R')
  library(pander)
  library(knitr)
  library(plyr)
  library(ggplot2)
  library(plm) # works on desktop at school
  library(stargazer)
  library(reshape)
  library(stats)
  library(rms)
  library(english)
  library(splm)
  library(spdep)
  library(rgdal)
  library(dplyr)
  library(svdvis)
  
  focus_group = read.csv('Focus Groups Summary.csv',stringsAsFactors = F)
  Table_number = 1
  Figure_number = 1
  Appendix_Table_number =1
  Appendix_Function_number =1
  Formula_number = 1

```

```{r SETUP: Read in data, message=FALSE, warning=FALSE, include=FALSE}
###########################################
# regressions on yields
  yield_ndvi = read.csv('H://Projects/India_Index_Insurance/India_Index_Insurance_Code/yield_ndvi.csv',stringsAsFactors = F)
  # yield_ndvi = read.csv('C://Users/mmann/Downloads/yield_ndvi (1).csv')
  yield_ndvi = yield_ndvi[!is.na(yield_ndvi$years),]
  
  yield_ndvi[yield_ndvi$i ==26 &yield_ndvi$years==2006,][2,] =NA
  yield_ndvi = yield_ndvi[!is.na(yield_ndvi$year),]
  yield_ndvi[ yield_ndvi$yield_tn_ha<1 | yield_ndvi$yield_tn_ha>6,'yield_tn_ha']= NA 
  
  # IMPORTANT: ORDER TO AVOID PROBLEMS WITH INDEX LATER  - plm sorts by name and year 
  yield_ndvi=yield_ndvi[with(yield_ndvi, order(years,district)), ]
  
  # Create variabels comparing percentiles to actual 
  yield_ndvi$rice_growing_95th_diff_mn =  yield_ndvi$rice_growing_mean-yield_ndvi$rice_growing_95th_prct
  yield_ndvi$VEG_annual_95th_diff_mx =  yield_ndvi$rice_growing_max-yield_ndvi$rice_growing_max_95th_prct
  yield_ndvi$VEG_annual_95th_diff_AUC =  yield_ndvi$rice_growing_AUC-yield_ndvi$rice_growing_AUC_95th_prct
  yield_ndvi=ddply(yield_ndvi, "district", transform, rice_growing_AUC_diff_mn = rice_growing_AUC-mean(rice_growing_AUC,na.rm=T))

  yield_ndvi$VEG_growing_95th_diff_mn = yield_ndvi$VEG_growing_mean-yield_ndvi$VEG_growing_95th_prct
  yield_ndvi$VEG_growing_95th_diff_mx = yield_ndvi$VEG_growing_max-yield_ndvi$VEG_growing_max_95th_prct
  yield_ndvi$VEG_growing_95th_diff_AUC =  yield_ndvi$VEG_growing_AUC-yield_ndvi$VEG_growing_AUC_95th_prct
  # rename to avoid omiting
  yield_ndvi$All_95th_prct =yield_ndvi$VEG_all_growing_95th_prct 
 
 
  # update names to seasons, Rabi = wheat, Kharif = rice
  names(yield_ndvi)=gsub("rice_growing","Kharif",names(yield_ndvi))
  names(yield_ndvi)=gsub("VEG_growing","Rabi",names(yield_ndvi))
  # remove all annual statistics use rice growing season instead
  yield_ndvi= yield_ndvi[,!(grepl('VEG',names(yield_ndvi)))]
  # final edits
  names(yield_ndvi)[names(yield_ndvi)=='rice_plant_dates']= 'Kharif_plant_dates'
  names(yield_ndvi)[names(yield_ndvi)=='rice_harvest_dates']= 'Kharif_harvest_dates'
  names(yield_ndvi)[names(yield_ndvi)=='plant_dates']= 'Rabi_plant_dates'
  names(yield_ndvi)[names(yield_ndvi)=='harvest_dates']= 'Rabi_harvest_dates'
  names(yield_ndvi)[names(yield_ndvi)=='season_length']= 'Rabi_season_length'

  names(yield_ndvi) 
```  
#Abstract 
The use of remote sensing for modeling and prediction of yields in developed countries has seen substantial academic interest. Here we aim to provide an open-source suite of algorithms to rapidly capture and summarize relevant portions of the phenological process relevant to crop modeling while maintaining spatio-temporal structure for use in panel econometric models.

#Introduction

The ability to monitor and predict crop yields in developing countries is critical to the successful adaptation to changes in our climate. Increased temperatures and variability has already been linked to losses in maize and wheat yields (-3.8 and 5.5% respectively)and crop prices globally [@Lobell616]. Although much effort has been placed on modeling the spatial distribution of these shifts, less effort has been placed on how yields vary across space and time [@Ray2015]. Advances in remote sensing provide new avenues to monitor agricultural crop health at high spatial and temporal resolution. However, our ability to monitor changes in plant productivity is still limited in the more complex environments common to many developing countries [@Mann2015]. 

One primary thrust of these efforts has evolved out of the index insurance space. Index insurance is...   

The main objectives of this project is to apply and compare statistical methods commonly applied these problems outside of the field of geography. In particular we will focus on the application of spatial panel regression to monitor spatio-temporal variability in wheat yields for Punjab and Haryana India.

#Methods
##Overview and Study Area

We examine wheat yields at the district level for Punjab and Haryana India for Rabi season (roughly Nov-Apr) for the period of `r paste(range(yield_ndvi$years, na.rm=T),collapse=' to ')`. Both Punjab and Haryana are extensively cropped but is comprised of a large number of smaller heterogeneous plots. Both states are also extensively double-cropped with rice planting in the Kharif season (roughly May-Oct) and Wheat planted in the Rabi season. Rabi season wheat yield range from `r paste(range(yield_ndvi$yield_tn_ha, na.rm=T),collapse=' to ')` metric tons per hectare (Table `r paste(Table_number)`, Figure `r paste(Figure_number)`).

*Figure `r paste(Figure_number)`: Mean Rabi Season Wheat Yields Metric Tons per Hectare by District `r Figure_number =Figure_number+1 `*
![Test](H:\Projects\India_Index_Insurance\India_Index_Insurance_Code\WriteUp\Yield_tn_ha.png)

 
Here we develop a (non)spatial panel regression model to estimate wheat output per hectare using the open-source programming language R. This model utilizes historical data on plant phenology statistics obtained from the Moderate Resolution Imaging Spectroradiometer (MODIS) satellite. The objective is to develop a handful of metrics that can be used to accurate predict inter-annual variability in wheat yields at the district level. 

##Data
The full model is comprised of `r paste(length(multi_grep_character(c('VEG','dates','EVI','NDVI','Rabi','Kharif','R_mx','All_95th'), names(yield_ndvi))))` indicators of plant phenology. District level statistics are then generated from pixel level plant indicators.

###Focus Group Interviews
To help better characterize the physical properties and identify challenges, field visits and focus group interviews were conducted in the winter of 2015. These interviews were conducted in 12 villages with 71 participants in Haryana and Pubjab states, in person with International Food Policy Research Institute (IFPRI) staff.  Questions focused on farm characteristics, adopted technologies, Rabi crop calendar dates, and identifying the timing of risks to crops. 

###Remote Sensing Data
Considering the relatively small scale of agriculture in this region (median plot sizes of `r paste(mean(13.5,10))` acres, range of 2 to 17.2 acres) reported during focus groups [@Robles2015], we utilized 250m vegetation products from the MODIS satellites. Vegetation indices are obtained from two 16 days MODIS products (MOD13Q1, MYD13Q1) from the Aqua and Terra satellites [@Didan2006]. Due to the staggered nature of acquisition these products are treated as partially overlapping windows representing 8 day periods [@doraiswamy2007crop]. 

In particular, we examine \ the predictive power of the Normalized Difference Vegetation Index (NDVI) using panel econometric techniques. NDVI is sensitive to the amount of chlorophyll in any given pixel and are commonly used to estimate plant productivity and health in agricultural applications [@Mann2015]. After removal of snow, cloud and other flagged low quality cells, we remove all non-agricultural cells through the use of the 500m MODIS land cover product (MCD12Q1) for the appropriate year [@Friedl2010]. The difference in resolutions is expected to have a minimal effect in this case because the extent of rural agriculture in these areas is extremely large. Therefore any cells include or excluded by omission or commission as agriculture should have a minimal effect at the district level. Moreover agricultural patterns are generally uniform over broad areas of Punjab and Haryana. 
 
In addition to cloud cover, MODIS data products suffer from four additional sources of error including atmospheric interference, georeferencing, bidirectional reflectance effect and differences in day of the year each pixel is observed [@doraiswamy2007crop]. While indices such as NDVI minimize the effects of atmospheric distortion but will directly influence indices values. To minimize the effects of the artifacts described above we test the use of temporal smoothing splines and outlier removal  [@hastie1990generalized]  **REFERENCE JOSHUA GRAY**. Where outliers above three standard deviations are removed before applying a cubic smoothing spline. A visual example of the effects of the cubic smoothing and outlier removal procedure can be seen in Figure `r paste(Figure_number)`. 

*Figure `r paste(Figure_number)`: (Un)smoothed 8-Day NDVI time signature for a dual cropped pixel in Punjab `r aa=Figure_number` `r Figure_number =Figure_number+1 `*
```{r, echo=FALSE,warning=F,fig.width=7, fig.height=5}
 rects = read.csv('./FigureTableData/rects.csv')
 rects$xend = as.Date(strptime(rects$xend,'%Y-%m-%d'))
 rects$xstart = as.Date(strptime(rects$xstart,'%Y-%m-%d'))
 plotdata = read.csv('./FigureTableData/NDVI_smooth_timeseries.csv')
 plotdata$Dates = as.Date(strptime(plotdata$Dates,'%Y-%m-%d'))
 harvest_lines = as.Date(strptime(read.csv('./FigureTableData/harvest_lines.csv',stringsAsFactors = F)[,2],'%Y-%m-%d'))
 plant_lines = as.Date(strptime(read.csv('./FigureTableData/plant_lines.csv')[,2],'%Y-%m-%d'))
 PlantHarvest =  read.csv('./FigureTableData/PlantHarvest.csv') 
 PlantHarvest$planting = as.Date(strptime(PlantHarvest$planting,'%Y-%m-%d'))
 PlantHarvest$harvest = as.Date(strptime(PlantHarvest$harvest,'%Y-%m-%d'))
 max_lines = as.Date(strptime(read.csv('./FigureTableData/max_lines.csv')[,2],'%Y-%m-%d'))

 
  vlinewidth = 0.50
  ggplot()+geom_rect(data = rects, aes(xmin = xstart, xmax = xend,
        ymin = -Inf, ymax = Inf), alpha = 0.4)+
        geom_line(data= plotdata[plotdata$Legend!='NDVI',], aes(x=Dates,y=NDVI,colour=Legend),linetype = 2,size=1.25)+
        geom_point(data= plotdata[plotdata$Legend=='NDVI',], aes(x=Dates,y=NDVI,colour=Legend),size=2)+
        geom_vline(colour='darkgreen',xintercept = as.numeric(as.Date(strptime(plant_lines,'%Y-%m-%d'))),size=vlinewidth)+
        geom_vline(colour='purple',xintercept = as.numeric(as.Date(strptime(harvest_lines,'%Y-%m-%d'))),size=vlinewidth)+
        geom_vline(colour='red',xintercept = as.numeric(as.Date(strptime(max_lines,'%Y-%m-%d'))),size=vlinewidth)+
        annotate("text", x =(PlantHarvest$planting[9]+35), y = 0.31, label = 'italic("Wheat")',angle = 90,colour='#3d4147',parse = T)+
        annotate("text", x =(PlantHarvest$harvest[9]+100), y = 0.31, label = 'italic("Rice")',angle = 90,colour='#3d4147',parse = T)+
        annotate("text", x =(PlantHarvest$planting[9]+30), y = 0, label = "italic(Peak)",angle = 90,colour='red',parse = T)+
        annotate("text", x =(PlantHarvest$planting[9]-60), y = 0, label = "italic(Plant)",angle = 90,colour='darkgreen',parse = T)+
        annotate("text", x =(PlantHarvest$planting[9]+133), y = .02, label = "italic(Harvest)",angle = 90,colour='purple',parse = T)+
        coord_cartesian(xlim = (as.Date(c("2010-02-01", "2015-02-25"))))
# ggsave(paste("./F",aa,"_NDVI time signature.pdf",sep=''))
 #windows() used in presentation
  #ggplot()+geom_rect(data = rects, aes(xmin = xstart, xmax = xend, ymin = -Inf, ymax = Inf), alpha = 0.4)+geom_line(data= plotdata[plotdata$Legend=='NDVI',], aes(x=Dates,y=NDVI,group=Legend),size=2)+coord_cartesian(xlim = (as.Date(c("2013-01-01", "2015-04-01"))))
 
```
*Time series of NDVI unsmoothed (red) and smoothed with outlier removal (blue) for both crop seasons of 2002 to 2016. Wheat growing season highlighted in dark grey, rice growing season in light grey. Date of estimated Rabi season maximum (red vertical line), estimated greenness onset (green vertical line), estimated harvest date (purple veritcal line).*

<!-- **To test for model improvements obtained through time series using cubic smoothing, we iteratively run a regression where the models described below (estimating wheat yields) with varying degrees of smoothing of inputs. For each iteration the degree of smoothing is increased and the adjusted $\ R^2$ and root mean squared error divided by mean yields is reported.** -->

###Agricultural Survey Data
Agricultural survey data at the district level was obtained from _________. INCLUDE DESCRIPTION OF SWAPPING PUNJAB STATE DATA.

##Exploiting Time: Summarizing Remotely Sensed Data
One of the primary challenges in utilizing an 8-day time series to estimate annual wheat yields is the mismatch in observations. Properties of the time signature must be obtained to characterize and identify important components of the plant phenology time signature correlated with wheat yields in these agricultural systems. Here we utilize `r paste(length(multi_grep_character(c('VEG','dates','EVI','NDVI','Rabi','Kharif','R_mx','All_95th'), names(yield_ndvi))))` metrics to summarize phenology. These measures take two primary forms: first, growing season statistics, spanning the estimated planting date of wheat (mean DOY:`r paste(round(mean(yield_ndvi$Rabi_plant_dates,na.rm=T)))`) until harvest date (mean DOY:`r paste(round(mean(yield_ndvi$Rabi_harvest_dates,na.rm=T)))`); and second monsoon season statistics, spanning end of the Rabi wheat season to end of the Kharif growing season. Two classes of statistics are estimated for these two periods: first, summary statistics (e.g. mean, max, variance), second, integrated summary statistics (e.g. area under the curve for the 1st 1/2 of the growing season), and third comparison to norms (e.g. comparisons to 95th percentile). 

Pixels with in area of interest (AOI), district boundaries in this case, can be evaluated on a pixel by pixel basis in utilizing parrallel processing or summarized by the AOI's mean value for each image. In this study, district-level mean values of NDVI are used to represent agricultural productivity for each 8-day period. The follow sections outline how these data are summarized for use in panel regression.

###Growing Season Metrics
Planting and harvest dates of are estimated for each growing season of interest. These dates are estimated through an iterative search algorithm finding the date of the global minimum NDVI value nearest to the *a priori* estimated date. A priori values were obtained from the focus group interviews described above. For wheat, sowing dates were reported to typically start in the last week of October, and harvest to begin in the 2nd week of April. For details on this see function `r A=Appendix_Function_number` `r paste(A)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix. Basic growing season summary statics including minimum, maximum, mean, and standard deviation can be calculated using function `r B=Appendix_Function_number` `r paste(B)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix below. 

To estimate the cumulative impact of high or low vegetation indices across as season we calculate a variety of integration metrics. These include area under the curve (AUC) of the growing season, the AUC of the increasing portion of the curve (from estimated planting date to growing season maximum), and the AUC of the declining portion of the curve (from growing season maximum to estimated harvest date). For comparision, these values are calculate using two methods, the first using integration using smoothing splines (appendix formula `r C=Appendix_Function_number` `r paste(C)` `r Appendix_Function_number=Appendix_Function_number+1`) and second using trapeziodal estimation (appendix formula `r D=Appendix_Function_number` `r paste(D)` `r Appendix_Function_number=Appendix_Function_number+1`). 

We develop a series of metrics to test if modeled yields could be improved through comparisons to 'ideal' years. This includes calculating the 95th percentile (based on sample quantile where the resulting quantile estimates are approximately median-unbiased regardless of the distribution of x [@hyndman1996sample]) of all NDVI values, of maximum values, and of the integral (area under the curve) of NDVI values.  These use the built in functionality of R's base stat function show formula `r E=Appendix_Function_number` `r paste(E)` `r Appendix_Function_number=Appendix_Function_number+1`) in the appedix. 

Additional functions were developed to extract the timing of particular phenomena, for instance the date of the maximum value of NDVI. Figure `r paste(aa)` visually demonstrates the ability of this function to estimate the timing on greeness onset (referred to henceforth as planting date), seasonal maximums, and harvest dates. For details on these calculations see formula `r F=Appendix_Function_number` `r paste(F)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix. Mutiway ties are handled by perfering the middle most date or if an even number of ties the left middle most date. Another calculates the average value of NDVI for each day of the year, which can be used for graphing anomolies (see formula `r G=Appendix_Function_number` `r paste(G)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix). Finally, some of the above codes have improved performance when run on smoothed time series while removing outliers. For this procedure we use a function developed by Joshua Gray at North Carolina State University (see function `r H=Appendix_Function_number` `r paste(H)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix). 

###Annual Metrics
Basic annual summary statics including minimum, maximum, mean, and standard deviation can be calculated using function `r I=Appendix_Function_number` `r paste(I)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix below. Alternatively most functions described above can be used to calculate annual vegetation metrics. 


###Variable Definitions
*Table `r paste(Table_number)`: Variable Names & Descriptions `r VarTableNum=Table_number` `r Table_number =Table_number+1 `*

```{r, echo=FALSE}
definitions = read.csv('Variable Description Table.csv')
#definitions
pander(definitions, justify = c('left', 'left'))
#pandoc.table(definitions, style = 'rmarkdown')
```

##Summarizing Space & Time: Extraction and Aggregation of Remotely Sensed Data
One major hurdle for this study was the rapid extraction of raster values bases on vector data while maintaining meaningful spatial and temporal components. In response, we developed the function *extract_value_point_polygon* (see formula `r J=Appendix_Function_number` `r paste(J)` `r Appendix_Function_number=Appendix_Function_number+1` in the appendix) to enhance the performance of the the default raster::extract() function. User processing times were better that 1/6000th that of extract() with the use of a 16-core linux server. Additionally the function can take a list of adjacent raster stacks to perform data extraction, thereby properly handling vector datasets that span more than the extent of one raster. 
 
*Figure `r paste(Figure_number)`: Benchmark test comparing raster::extract() and the new parrallelized extract functions `r b7839=Figure_number` `r Figure_number =Figure_number+1`*

```{r, echo=FALSE}
bench = read.csv('FigureTableData/benchmark_extract.csv',stringsAsFactors = F)
bench_extract_time = bench[1,2] # remove text data
bench = bench[2:dim(bench)[1],]
bench$cores = as.numeric(bench$cores)
ggplot(data=bench,aes(x=cores,y=times,group=NULL))+geom_line(size=1)+geom_hline(yintercept=bench_extract_time,colour='red',size=1)+xlab('Cores')+ylab('User Time')+ annotate("text", x = 7, y = 150, label = "extract_value_point_polygon")+ annotate("text", x = 7, y = 287, label = "raster::extract", col='red')
#ggsave(paste("./F",b7839,"_ExtractBenchmark.pdf",sep=''))

```



```{r, echo=FALSE}
# not sure we include this one 
 Neighborhood_quantile=function(extr_values,PlantHarvestTable,Quant_percentile=0.05,num_workers=5,spline_spar = 0){
     # take in values from extract_value_polygon and returns quantile for all raster values wihtin poly
     # if spline_spar = 0, doesn't smooth data, as spline_spar increases smoothing decreases

     # iterate between spatial objects
     registerDoParallel(num_workers)
     result_summary=foreach(i = 1:length(extr_values),.packages='raster',.inorder=T) %dopar%{
        if(is.na(extr_values[[i]])){ print('Empty Object');return(NA)} # avoid empties

        # Get dates from stack names
        dats = strptime( gsub("^.*X([0-9]+).*$", "\\1", names(extr_values[[i]])),format='%Y%j')
        # Calculate smoothed values
        if(spline_spar!=0){
        smooth = lapply(1:dim(extr_values[[i]])[1],function(z){SplineAndOutlierRemoval(
            x = as.numeric(extr_values[[i]][z,]),
            dates=as.Date(dats),
            pred_dates=as.Date(dats),spline_spar)})}else{
            smooth = lapply(1:dim(extr_values[[i]])[1],function(z) as.numeric(extr_values[[i]][z,]))    }
	# calculate quantile for all values over polygon 
        N_Qnt = quantile(x = unlist(smooth),p=Quant_percentile,type=8,na.rm=T)
        N_Qnt     
      }
    result_summary
 }
```


##Exploiting Time: (Spatial) Panel Regression Methods and Models
<!-- ###Input Data -->
<!-- A summary table of the value of the dependent variable and all independent variables can be found below: -->

<!-- *Table `r paste(Table_number)`: Variable summary statistics and descriptions `r Table_number =Table_number+1 `* -->

<!-- ```{r Input variable summary table, echo=FALSE} -->
<!--   panderOptions('table.split.table', Inf) -->
<!--   panderOptions('big.mark', ",") -->
<!--   panderOptions('keep.trailing.zeros', F) -->
<!--   table.yield_ndvi = yield_ndvi[,10:dim(yield_ndvi)[2]] -->
<!--   table.yield_ndvi.out = data.frame(Mean = sapply(table.yield_ndvi, mean, na.rm=T),SD=sapply(table.yield_ndvi, sd, na.rm=T) ) -->
<!--   pander(table.yield_ndvi.out, justify = c('left', 'center', 'center')) -->
<!-- ``` -->

##Panel Regression Methods and Diagnostic results

```{r SETUP: Calculate PCA, echo=FALSE, message=FALSE, warning=FALSE}
  
  PCA_input_formula =    yield_tn_ha  ~   Rabi_plant_dates + Rabi_harvest_dates + Rabi_season_length + Rabi_max_date + Rabi_mean + Rabi_min + Rabi_max + Rabi_AUC + Rabi_95th_prct + Rabi_max_95th_prct + Rabi_AUC_95th_prct + Rabi_AUC_v2 + Rabi_AUC_leading + Rabi_AUC_trailing + Rabi_AUC_diff_mn + Rabi_AUC_diff_90th + Rabi_sd + Kharif_plant_dates + Kharif_harvest_dates + R_mx_dates + Kharif_mean + Kharif_min + Kharif_max + Kharif_AUC + Kharif_95th_prct + Kharif_max_95th_prct + Kharif_AUC_95th_prct + Kharif_AUC_v2 + Kharif_AUC_leading + Kharif_AUC_trailing + Kharif_95th_diff_mn + Kharif_AUC_diff_mn + Rabi_95th_diff_mn + Rabi_95th_diff_mx + Rabi_95th_diff_AUC + All_95th_prct  
  
  PCA_input_formula_dataframe =    yield_tn_ha  ~   Rabi_plant_dates + Rabi_harvest_dates + Rabi_season_length + Rabi_max_date + Rabi_mean + Rabi_min + Rabi_max + Rabi_AUC + Rabi_95th_prct + Rabi_max_95th_prct + Rabi_AUC_95th_prct + Rabi_AUC_v2 + Rabi_AUC_leading + Rabi_AUC_trailing + Rabi_AUC_diff_mn + Rabi_AUC_diff_90th + Rabi_sd + Kharif_plant_dates + Kharif_harvest_dates + R_mx_dates + Kharif_mean + Kharif_min + Kharif_max + Kharif_AUC + Kharif_95th_prct + Kharif_max_95th_prct + Kharif_AUC_95th_prct + Kharif_AUC_v2 + Kharif_AUC_leading + Kharif_AUC_trailing + Kharif_95th_diff_mn + Kharif_AUC_diff_mn + Rabi_95th_diff_mn + Rabi_95th_diff_mx + Rabi_95th_diff_AUC + All_95th_prct + years + district
  
     ###########################
  # USE PCA 
  pca_input = na.omit(model.frame(PCA_input_formula_dataframe,yield_ndvi))
  #pca_data = pca_input[,sapply(pca_input,is.numeric)] # limit to numeric number data
  pca_data = pca_input[,!(names(pca_input) %in% c('area','production_tonnes','yield_tn_ha','district','years' ))] # remove dependent variable data
  pca = prcomp( pca_data, scale = T,center = T ) 
 
  pca_pred = as.data.frame(stats::predict(pca))
  pca_pred$district = pca_input$district
  pca_pred$years = pca_input$years
  pca_pred$yield_tn_ha =pca_input$yield_tn_ha
  
   # IMPORTANT: ORDER TO AVOID PROBLEMS WITH INDEX LATER 
  pca_pred=pca_pred[with(pca_pred, order(district, years)), ]

  # transformed variables 
  PCA_output_formula =yield_tn_ha~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10 + PC11 + PC12 + PC13 + PC14 + PC15
  PCA_output_formula_dataframe = yield_tn_ha~PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10 + PC11 + PC12 + PC13 + PC14 + PC15+district+years 
```

 
 
```{r Panel: Spatial autocorrelation, message=FALSE, warning=FALSE, include=FALSE}
    
  #distrct boundaries
  districts_lm = readOGR('H:/Projects/India_Index_Insurance/Data/Admin Boundaries','PunjabHaryanaDistricts')
  districts_lm = spTransform(districts_lm, CRS('+proj=sinu +a=6371007.181 +b=6371007.181 +units=m'))
  districts_lm$NAME_2 = toupper(as.character(districts_lm$NAME_2)) 
        
  # Create spatial weights  Remove all districts not in yield_ndvi
  districts_lm = districts_lm[as.character(districts_lm@data$NAME_2) %in% as.character(unique(yield_ndvi[,'district'])),]
  districts_polyNB_lm = poly2nb(districts_lm,row.names = row.names(districts_lm)) # polygon continuity$GEOID10
  Wneigh_lm = nb2mat(districts_polyNB_lm, style='W')
  queen = nb2listw(districts_polyNB_lm) 
  # k nearest weights 
  centroids = coordinates(districts_lm)
  knn1 = nb2listw(knn2nb(knearneigh(centroids, k = 1), row.names = districts_lm$NAME_2 ))
  knn2 = nb2listw(knn2nb(knearneigh(centroids, k = 2), row.names = districts_lm$NAME_2 ))
  knn3 = nb2listw(knn2nb(knearneigh(centroids, k = 3), row.names = districts_lm$NAME_2 ))
  knn4 = nb2listw(knn2nb(knearneigh(centroids, k = 4), row.names = districts_lm$NAME_2 ))
  knn5 = nb2listw(knn2nb(knearneigh(centroids, k = 5), row.names = districts_lm$NAME_2 ))
  knn6 = nb2listw(knn2nb(knearneigh(centroids, k = 6), row.names = districts_lm$NAME_2 ))

  OLS = lm(PCA_output_formula, data=pca_pred)
  OLS_resid = data.frame(model.frame(PCA_output_formula_dataframe,pca_pred)[,c('years','district')], residuals= OLS$residuals)
  OLS_resid = aggregate(residuals~district,data =OLS_resid, function(x){mean(x,na.rm=T)})
  moran_queen = moran.test(OLS_resid$residuals,queen)
  moran_knn1  = moran.test(OLS_resid$residuals,knn1)
  moran_knn2  = moran.test(OLS_resid$residuals,knn2)
  moran_knn3  = moran.test(OLS_resid$residuals,knn3)
  moran_knn4  = moran.test(OLS_resid$residuals,knn4)
  moran_knn5  = moran.test(OLS_resid$residuals,knn5)
  moran_knn6  = moran.test(OLS_resid$residuals,knn6)

  Moran_results = data.frame(pvalue = c(moran_queen$p.value,moran_knn1$p.value,moran_knn2$p.value,moran_knn3$p.value,moran_knn4$p.value,moran_knn5$p.value,moran_knn6$p.value),Neighborhood =c('Polygon Continuity',paste('KNN',1:6,sep=' ')), Test_stat =c(moran_queen$statistic[[1]],moran_knn1$statistic[[1]],moran_knn2$statistic[[1]],moran_knn3$statistic[[1]],moran_knn4$statistic[[1]],moran_knn5$statistic[[1]],moran_knn6$statistic[[1]]) )
  
  #Baltagi, Song and Koh LM2 marginal test
   knn6 = (knn2nb(knearneigh(centroids, k = 6), row.names = districts_lm$NAME_2 ))
   
  # find all districts with 10 years
  bskdata = data.frame(district=pca_pred$district,years=pca_pred$years,as.data.frame(pca_pred))
  balanced_panel = as.character(as.data.frame(table(bskdata$district))$Var1[as.data.frame(table(bskdata$district))$Freq ==10])
  table(bskdata$years[bskdata$district %in% balanced_panel])
  bskdata_balanced = bskdata[bskdata$district %in% balanced_panel,]
  bskdata_balanced = model.frame(PCA_output_formula_dataframe,bskdata_balanced)
  
  # get spatial weights for balanced panel
  districts_lm = districts_lm[as.character(districts_lm@data$NAME_2) %in% as.character(unique(bskdata_balanced[,'district'])),]
  districts_polyNB_lm = poly2nb(districts_lm,row.names = row.names(districts_lm)) # polygon continuity$GEOID10
  Wneigh_lm = nb2mat(districts_polyNB_lm, style='W')
  queen = mat2listw(Wneigh_lm) 
  
  centroids = coordinates(districts_lm)

  knn6 = mat2listw( nb2mat( knn2nb(knearneigh(centroids, k = 4), row.names = districts_lm$NAME_2 )) )

  bsktest(x=PCA_output_formula, data=bskdata_balanced, listw=queen,test='CLMlambda',index = c('district','years'))
  
  #getAnywhere('bsktest.formula')  # view function 
```  

###Diagnostic Tests 
To avoid overstating the statistical significance of regression coefficients we test the residuals from a pooled linear for spatial autocorrelation [@Fotheringham2002]. Here we estimate equation `r F1=Formula_number` `r paste(F1)`: `r Formula_number=Formula_number+1`

(`r paste(F1)`)                
$\ Y_{it}=\alpha+\sum_{a=1}^{k}\beta_{k}X_{it}+ \mu_{it}$

Where $\ Y_{it}$ is a vector of our dependent variable, district yields in tons per hectare (*yield_tn_ha*), for each district *i* for each year *t*, $\ \alpha$ is an intercept term, $\ \beta_k$ is a vector of *k* coefficients, $\ X_{it}$ corresponds to the *k* independent variables describing NDVI, and $\ \mu_{it}$ is pooled error term.

Because a better implementation of the Moran's I for panel regression residuals is not available, tests for spatial autocorrelation are run on the mean regression residuals $\overline{\mu_{i}}$ for each district (*i*)  [@Cliff1981]. Tests were carried out for seven potential neighborhood definitions including six different measures of K-nearest Neighbor, and a queen's polygon continuity (Figure `r paste(Figure_number)`). Moran's I test statistics are less than `r round(mean(Moran_results$Test_stat),2)`, and fail to reject (p =`r round(min(Moran_results$pvalue),2)`) the null hypothesis of spatial independence. This would be expected however since $\overline{\mu_{i}}$ is by definition pulled towards the mean, and thus masking any spatial variability. Instead of relying on tests for spatial autocorrelation we will instead evaluate the ability of spatial models to impact model accuracy. 
  
*Figure `r paste(Figure_number)`: Statistical significance of Moran's I on pooled regression residuals`r c234=Figure_number` `r Figure_number =Figure_number+1 `*
```{r Panel: Spatial autocorrelation plot, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(data=Moran_results,aes(y=pvalue,x=factor(Neighborhood),group=Neighborhood,fill=Neighborhood))+ geom_bar(stat = "identity")+geom_hline(yintercept = 0.05,colour='grey',size=1.5,linetype=5)+ scale_y_continuous(breaks=c(0,0.05,.1,.2,.4,.6))+ theme(legend.position="none",axis.text.x  = element_text(angle=90, vjust=0.5))+labs(x = 'Neighborhood Type', y='P-Value')
#ggsave(paste("./F",c234,"_Morans_Pvalue.pdf",sep=''))
``` 


####Tests: Pooled, Fixed or Random  
```{r Panel: Hausman test, echo=FALSE, message=FALSE, warning=FALSE}
# run hausman test
  
  # run fixed and random effects
  fixed <- plm(PCA_output_formula, data=pca_pred, index=c("district", "years"), model="within")
  random <- plm(PCA_output_formula, data=pca_pred, index=c("district", "years"), model="random")
  pooled <- plm(PCA_output_formula, data=pca_pred, index=c("district", "years"), model="pooling")
 # znp <- pvcm(formula2,data=yield_ndvi,model="within") # not enough obs to run 

 
 # test_pool1=pooltest(znp,fixed)  #F test It is a standard F test, based on the comparison of a model obtained for the full sample and a model based on the estimation of an equation for each individual. null:  test the hypothesis that all the coefficients are equal (pooled OLS better than fixed)  https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf
  
  test_pool2=pooltest(pooled,fixed) # null:  pooled OLS better than fixed
    
  test_pool_random = plmtest(pooled, type=c("bp")) #The null hypothesis in the LM test is that variances across entities is zero (RE inappropriate). This is, no significant difference across units (i.e. no panel effect), ALT; RE better http://www.princeton.edu/~otorres/Panel101R.pdf
  
  test_hausman_fixed_random = phtest(fixed, random) # use fixed if significant To decide between fixed or random effects you can run a Hausman test where the null hypothesis is that the preferred model is random effects vs. the alternative the fixed effects (see Green, 2008, chapter 9). It basically tests whether the unique errors (ui ) are correlated with the regressors, the null hypothesis is they are not. http://www.indiana.edu/~wim/docs/10_7_2011_slides.pdf
  #pander(hausman)
```

Testing is required to choose the proper estimation method for panel regression. We must choose between pooled, fixed effect (FE), and random effect (RE) models. First we can test for poolability of our model. Pooled regression assumes a constant intercept and slopes between different districts and time periods. We can test if variance across districts is equal to zero using an F-test. Here we reject the use of pooled Ordinary Least Squares (OLS) (p <= `r sprintf("%11.2e",test_pool2$p.value)`) in favor of a fixed effects model with unique intercepts for each district. We can then compare the use of the fixed effects and random effects models. The hausman test checks for exogeniety of the unobserved error component, if the null hypothesis is rejected, the random effects model is inconsistent, and the fixed effects model will be preferred. If individual effects are exogenous both fixed and random effects are asymptotically equivalent. Here we test if $\ H_{0}:\hat{\beta}_{RE}=\hat{\beta}_{RE}$, where $\hat{\beta}_{RE}$ are coefficient vectors of time-varying explanitory variables. We reject the null hypothesis  (p <= `r sprintf("%11.2e",test_hausman_fixed_random$p.value) `) and choose to use the fixed effects estimator as it will be the only consistent estimator.  
 
####Multicolinearity & Principal Components Analysis Transform
Multicolinearity, high correlations between independent variables, can increase estimates of a variable's estimated variance. This can have the adverse effect of creating models in which the $\ R^2$ is high and no variables are statistically significant. Multicolinearity can also produce coefficients of the 'wrong sign' and of unreasonable magnitude [@o2007caution; @greeneh]. Here we use a variance inflation factor (VIF) to quantify how much the variance is inflated for each coefficient[^1]. VIF values over 4 for any variable are generally considered problematic and require further examination [@greeneh]. Here we present summary statistics for VIFs on a ordinary least squares estimation of all model variable in Table (`r paste(Table_number)`) below:

[^1]: A nice synopsis of VIF can be found here [@State2016].

*Table `r paste(Table_number)`: Variance inflation factor summary table `r Table_number =Table_number+1 `*

```{r Panel: Multicolinearity test (vif),echo=F}
  #https://onlinecourses.science.psu.edu/stat501/node/347
  fit =lm(PCA_input_formula,data=yield_ndvi)
  fit = as.data.frame(vif(fit))
  stargazer(fit,type = 'text')
```

To avoid problems with multicolinearity between independent variables we apply a principal components analysis transformation (PCA) to a centered and scaled matrix of all independent variables $\ X$. PCA allows for the replacement of $\ X$ with a new matrix whos variables are orthogonal to each other but span the multidimensional space of $\ X$ [@GELADI19861]. In this case, we generate `r paste(dim(pca$rotation)[1])` principal components for inclusion in a random effects panel regression. A table of PCA component imporance can be found in the appendix, Table(A`r AT1=Appendix_Table_number``r paste(AT1)``r Appendix_Table_number=Appendix_Table_number+1`).
 
 
###OLS Regression Estimation
```{r Balance the panel, message=FALSE, warning=FALSE, include=FALSE}

  # BALANCE THE PANEL
  # find all districts with 10 years
  balanced_panel = as.character(as.data.frame(table(pca_pred$district))$Var1[as.data.frame(table(pca_pred$district))$Freq ==10])
  
  # confirm balanced 
  table(pca_pred$years[pca_pred$district %in% balanced_panel])
  pca_pred = pca_pred[pca_pred$district %in% balanced_panel,]
                                                
  # IMPORTANT: ORDER TO AVOID PROBLEMS WITH INDEX LATER  FOR SPLM MUST BE SORTED BY YEARS DISTRICT
  pca_pred=pca_pred[with(pca_pred, order(years, district )), ]        
```



```{r LM for prediction, message=FALSE, warning=FALSE, include=FALSE}
  # formulas

  PCA_formula_regression =yield_tn_ha~   PC1+PC2+PC3+PC4 
  
  PCA_formula_regression_dataframe = yield_tn_ha~PC1+PC2+PC3+PC4+district+years   
  
  
  # estimate lm
  lm1 <- lm(PCA_formula_regression, data=pca_pred)
 
  #stargazer(fixed_pca, type="text")
  summary(lm1)
   
 
  
  # predict lm to all dates
  lm_pred = predict(lm1,newdata = pca_pred,se.fit=T)
   
  # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709
  SSR_FULL = sum((pca_pred$yield_tn_ha-lm_pred$fit)^2)
  SSR_FE = sum( lm(yield_tn_ha ~ 1 +as.factor(district) , data=pca_input)$residuals^2)
  Witin_R2 =  1 - (SSR_FULL/SSR_FE)  
  print(paste('Within R2',round(Witin_R2,2)))

   # get prediction and and model.frame 
  model_data_lm = data.frame(pca_pred,LM_Fit = lm_pred$fit)
  model_data_lm$district = as.character(model_data_lm$district)
  model_data_lm$years_id = as.numeric(substr(model_data_lm$year,1,4))
  model_data_lm = model_data_lm[,c('district','years_id','yield_tn_ha','LM_Fit')]
  model_data_lm = melt(model_data_lm,id = c('years_id','district'))
  
```  

Traditional ordinary least squares (OLS) approaches look at cross-sectional (or time-series) data, exploiting variance in one dimension. However most social and phsyical processes occur over both space and time. For this reason we focus on the use of panel data sets which increases the amount of observed heterogeniety by including information about individuals *i* over time *t*. Critically, despite its widespread use in the field Geography (e.g. [@Mann2010a]), cross-sectional should *not* be used for forecasting, or for modeling of phenomenon with strong temporal components. 

Compared to cross-sectional approaches, panel analysis substantially increases the degree of observed variance over both space and time. When pooled together, the integration of two statewide data sets provides (n = `r paste(length(unique(model.frame(PCA_formula_regression_dataframe,pca_pred)[,'district'])))`) over the `r paste(range(as.numeric(as.character(pca_pred$years)), na.rm=T),collapse='-')` sample period (t = `r paste(length(unique(model.frame(PCA_formula_regression_dataframe,pca_pred)[,'years'])))`) provides (N=`r  length(unique(model.frame(PCA_formula_regression_dataframe,pca_pred)[,'years']))*length(unique(model.frame(PCA_formula_regression_dataframe,pca_pred)[,'district']))`) observations. Due to issues with estimating spatial panel models, the input data must be balanced (no missing observations for any districts). As such the number of districts for this regression is limited to **14** out of **36** total. This loss however is compensated by the fact that we can now compare a variety of estimation stategies on an even playing ground. **Additionally, the omitted 22 districts will be used for out of sample testing later.**

Pooled OLS estimation assumes that intercepts and slopes are fixed between individual districts *i* with no specific treatment of time *t*. Although simple to understand, pooled OLS sometimes fails to properly control for determinants of spatial and temporal heterogeniety (e.g. districts with different policies, or changes to policies over time). Although the choice between pooled vs more complicated estimators, must be tested (see section '*Tests: Pooled, Fixed or Random*'). For comparison we estimate three types of panel models in this paper. The first of which, pooled OLS, is estimated in equation `r F2=Formula_number` `r paste(F2)`: `r Formula_number=Formula_number+1`

(`r paste(F2)`)                
$\ Y_{i}=\alpha+\sum_{k=1}^{K}\beta_{k}PC(k)_{i}+\epsilon_{i}$

Where $\ Y_{i}$ is a vector of our dependent variable, district yields in tons per hectare (*yield_tn_ha*) for each observation *i* which includes all districts across all years, $\ \alpha$ is an intercept term, $\ \beta_k$ is a vector of *K* coefficients, $\ X_{i}$ corresponding to the *K* principal components, and $\ \epsilon_{i}$ is the residual.  

###Panel Regression Estimation

We use panel data to model wheat yields over time at the district level. The use of panel data in this study helps to alleviate two key problems, unobserved spatial and temporal dynamics, and homogeneity (lack of variance). Now we use *i* to designate districts, rather than individual observations, and individual observations are the unique combination of *i* and *t*. 

For a fixed effect panel, we estimate equation `r F3=Formula_number` `r paste(F3)`: `r Formula_number=Formula_number+1`

(`r paste(F3)`)                
$\ Y_{it}=\alpha_{i}+\sum_{k=1}^{K}\beta_{k}PC(k)_{it}+ \mu_{it} +\epsilon_{it}$
 
Where $\ Y_{it}$ is a vector of our dependent variable, district yields in tons per hectare (*yield_tn_ha*) for each district *i* for each year *t*, $\ \alpha_{i}$ are n-1 intercept terms and control for unobserved characteristics of each district *i*, $\ \beta_k$ is a vector of *K* coefficients, $\ X_{it}$ corresponding to the *K* principal components, $\ \mu_{it}$ is the between entity error term, and $\ \epsilon_{it}$ is the within entity error term.  

[^2]: The first four principal components in this PCA comprises `r paste(round(summary(pca)$importance[3,4]*100))` % of total variance in $\mathbf{X}$

###Spatial Panel Regression Estimation
Spatial autocorrelation is a special case of cross-sectional dependence caused by similarities of neighboring districts, and creates a situation whereby data can no longer be considered independently generated [@Anselin1999;@Elhorst2010]. The inclusion of a spatial lag (spillovers) can also increase predictive accuracy, as neighboring regions are often effected by similar exogenous shocks (for instance drought or rust) [@Mann2014;@Mann2017;@Mann2015]. A spatial lag model can be considered a specification identifying the equilibrium outcome of spatial or social interaction processes, where the dependent variable for an individual is jointly determined with that of its neighbors [@Elhorst2017].

For these reasons a spatially lagged fixed effect panel model is developed where spatial dependence is controlled for using a spatially weighted dependent variable, in the following form in `r F4=Formula_number` `r paste(F4)`: `r Formula_number=Formula_number+1`

(`r paste(F4)`)                
$\ Y_{it}=...+\lambda\sum_{j=2}^{J}W_{ij}Y_{jt}+...+ \gamma_{t} +\epsilon_{it}$

Formula `r Formula_number-1` augments the specification in `r Formula_number-1`. The primary difference is the inclusion of $\lambda\sum_{j=2}^{J}W_{ij}Y_{jt}$ where $\lambda$ is called the spatial autoregressive coefficient, $W_{ij}$ is a row standardized weights matrix based for each individual *i* for its neighbors *j* on **polygon continuity**. These weighted values can be considered the mean values of 'neighboring' districts.  

##Cross Validation
In predictive applications is it also extremely important to provide relevant measures of predictive performance. In particular for this application we are interested in the ability of the model to estimate a wheat yields based on a new growing season's data or for an unknown value within the sample. First to validate the accuracy of our model we complete a Leave-P-Out Cross Validation (LPOCV), where each of the models `r paste(F2,F4,sep='-')` are reestimated, withholding a single year of observations, and storing the residuals. This process is repeated *p* times until all years have been evaluated. We then calculate the Root Mean Squared Error (RMSE) of the retained residuals, providing an estimate of the predictive accuracy of our models. RMSE is defined by `r F5=Formula_number` `r paste(F5)`: `r Formula_number=Formula_number+1`

(`r paste(F5)`)                
$\ RMSE=\sqrt{\frac{\sum_{n=1}^{N}y_{t}-\hat{y}_{t}}{n}}$

Where $y_{t}$ are observed values of *yield_tn_ha* and $\hat{y}_{t}$ are the predicted values for time $t$. 

The following results section will outline the results from OLS, Panel, and Spatial Panel estimation (e.q. `r paste(F2,F4,sep='-')`)

#Results
##Focus Group Interviews
 
##Principal Component Loadings 
Next we can evaluate how each variable contributes to the principal component used in later in this study. Large loadings indicate that a variable has a large effect on a principal component, this can be either in the positive or negative direction. We can see in Figure `r paste(Figure_number)` below for principal component one (PC1), a large and positive loading for *Kharif_95th_diff_mn*, a difference between  mean NDVI for year *i*  and the 95th percentile of historical values. This is relative to the large and positive loadings of a variety of variables largely relating to upper percentiles of the Kharif and Rabi seasons such as *Rabi_max & Kharif_max* and *Kharif_95th_prct*, a measure of the 95th percentile for that season. Combined this component emphasizes the variability between Kharif and Rabi growing seasons. The second principal component (PC2) is primarily positve loadings of the lower percentiles of the distribution for both growing seasons including *Kharif* and *Rabi_min*. PC3 has strong loadings for variables relating to planting and harvest dates including *Kharif_harvest_dates*, *Rabi_plant_dates* and *Rabi_season_length*. 

*Figure `r paste(Figure_number)`: Principal Component Analysis Loadings`r aa=Figure_number` `r Figure_number =Figure_number+1 `*

```{r, echo=FALSE,warning=F,fig.width=8, fig.height=5}

  source('H:/Scripts/visualize_pca_rotations.R')
  visualize_pca_rotations(pca, r=4, low='#76448A', mid ='#f7f7f7' , high='#1E8449' )

```

##Panel Regression
In this section we compare the results of three panel regression estimation techniques: pooled, fixed effects and spatially lagged fixed effects. In particular we are interested in the adjusted $R^{2}$ which controls for the loss of degrees of freedom, and the 'within' $R^{2}$ which evaluates the goodness of fit beyond what can be explained by fixed effects intercepts $\alpha_{i}$ (or transform), (see "Assessing goodness of fit" in [@Stata2016]). This 'within' $R^{2}$ is of particular interest because it is our best estimate of performance of the model in estimating year to year variations in crop yields. Since the objective of this study is to make predictions of wheat yields per hectare, it is also important to look at the accuracy of our estimation in these units. For this reason we also report the Root Mean Square Error (RMSE).
###Pooled OLS
Estimates of equation (`r paste(F2)`) are provide below:

Table `r paste(Table_number)`: *District level pooled estimation of wheat yields in tons per hectare*`r t543 =Table_number ` `r Table_number =Table_number+1 `


```{r Panel POOLED: Fitted vs actual PCA estimate, echo=FALSE}
summary(lm1)
SSR_FULL = sum(lm1$residuals^2)
SSR_FE = sum( plm(yield_tn_ha ~ 1 +as.factor(district) , data=pca_input, index=c("district", "years"), model="pooling")$residuals^2)
Witin_R2_pool =  1 - (SSR_FULL/SSR_FE)  
print(paste('Within R2',round(Witin_R2_pool,2)))
RMSE_lm1 = sqrt(sum((unlist(lm1$residuals))^2)/length(lm1$residuals))
print(paste('RMSE',round(RMSE_lm1,2)))
```

### Fixed Effects Panel

Estimates of equation (`r paste(F3)`) are provide below:

Table `r paste(Table_number)`: *District level fixed effects estimation of wheat yields in tons per hectare*`r t632 =Table_number ` `r Table_number =Table_number+1 `


```{r Panel FE: Fitted vs actual PCA estimate, echo=FALSE}

  
  # add a time lag  or dif
  pca_pred <- pdata.frame(pca_pred, index = c("district", "years"))

  # estimate plm
  fixed_pca <- plm(PCA_formula_regression, data=pca_pred, index=c("district", "years"), model="within")
 
  #stargazer(fixed_pca, type="text")
  summary(fixed_pca)
   
  # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709
  SSR_FULL = sum(fixed_pca$residuals^2)
  SSR_FE = sum( plm(yield_tn_ha ~ 1 +as.factor(district) , data=pca_input, index=c("district", "years"), model="pooling")$residuals^2)
  Witin_R2_FE =  1 - (SSR_FULL/SSR_FE)  
  print(paste('Within R2',round(Witin_R2_FE,2)))
  
  RMSE_pca = sqrt(sum((unlist(fixed_pca$residuals))^2)/length(fixed_pca$residuals))
  print(paste('RMSE',round(RMSE_pca,2)))

  # get prediction and and model.frame  from FE PCA
  fitted_pca = data.frame(PCA_Fit = fixed_pca$model[[1]] - fixed_pca$residuals)
  model_data_pca = cbind(as.data.frame(as.matrix(fixed_pca$model)),fitted_pca)
  model_data_pca = cbind(model_data_pca,na.omit(model.frame(PCA_formula_regression_dataframe,pca_pred)))
  model_data_pca$district = as.character(model_data_pca$district)
  model_data_pca$years_id = as.numeric(substr(model_data_pca$year,1,4))
  model_data_pca = model_data_pca[,c('district','years_id','yield_tn_ha','PCA_Fit')]
  model_data_pca = melt(model_data_pca,id = c('years_id','district'))
```


##Spatial Panel Regression

Estimates of equation (`r paste(F4)`) are provide below:
Table `r paste(Table_number)`: *District level spatial lag fixed effects estimation of wheat yields in tons per hectare*`r t098 =Table_number ` `r Table_number =Table_number+1 `
```{r Panel: Spatial Panel Estimation, fig.width=10, fig.height=6, echo=FALSE}
  #yeild and evi data
  pca_pred_splm = as.data.frame(pca_pred)

  #distrct boundaries
  districts_plm = readOGR('H:/Projects/India_Index_Insurance/Data/Admin Boundaries','PunjabHaryanaDistricts',verbose = F)
  districts_plm = spTransform(districts_plm, CRS('+proj=sinu +a=6371007.181 +b=6371007.181 +units=m'))
  districts_plm$NAME_2 = toupper(as.character(districts_plm$NAME_2)) 
  
 # get District outlines
  districts_plm = readOGR('H:/Projects/India_Index_Insurance/Data/Admin Boundaries','PunjabHaryanaDistricts',verbose=F)
  districts_plm = spTransform(districts_plm, CRS('+proj=sinu +a=6371007.181 +b=6371007.181 +units=m'))
  districts_plm$NAME_2 = toupper(as.character(districts_plm$NAME_2)) 
        
  # Create spatial weights  Remove all districts not in balanced panel
  districts_plm = districts_plm[as.character(districts_plm@data$NAME_2) %in% as.character(unique(pca_pred[,'district'])),]
  districts_polyNB = poly2nb(districts_plm
                             ,row.names=as.character(districts_plm@data$NAME_2)) # row.names = row.names(districts_plm)polygon continuity$GEOID10
  Wneigh = nb2mat(districts_polyNB, style='W')
  districts_polyListw = nb2listw(districts_polyNB) 
  
  
 # SPLM - Spatial lag estimation  -------------------------------------------------
   
  splm_lag <- spml(PCA_formula_regression, data = pca_pred, index = c("district", "years"),
                     listw = districts_polyListw, model = "within", lag = T, spatial.error = "none")
  summary(splm_lag)
  
  paste('R2=',summary(splm_lag)$rsqr)
  
  # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709
  SSR_FULL = sum(splm_lag$residuals^2)
  SSR_FE = sum( spml(yield_tn_ha~1 +as.factor(district), data = pca_pred, index = c("district", "years"),
                     listw = districts_polyListw, model = "pooling", lag = F, spatial.error = "b")$residuals^2)
  Witin_R2_splm =  1 - (SSR_FULL/SSR_FE)    
  paste('Within R2=',round(Witin_R2_splm,2))
  RMSE_lag = sqrt(sum((unlist(splm_lag$residuals))^2)/length(splm_lag$residuals))
  print(paste('RMSE',round(RMSE_lag,2)))
  
  pca_pred=pca_pred[with(pca_pred, order(years, district )), ]  # must do to do prediction 
  # plot spatial lag panel regression 
  model_pca_splm_lag = data.frame(SPLM_Fit =  splm_lag$model[[1]] - splm_lag$residuals )
  model_pca_splm_lag = cbind(as.data.frame(as.matrix(splm_lag$model)),model_pca_splm_lag)
  model_pca_splm_lag = cbind(model_pca_splm_lag,na.omit(model.frame(PCA_formula_regression_dataframe,pca_pred)))  
  
  model_pca_splm_lag$district = as.character(model_pca_splm_lag$district)
  model_pca_splm_lag$years_id = as.numeric(substr(model_pca_splm_lag$year,1,4))
  model_pca_splm_lag = model_pca_splm_lag[,c('district','years_id','yield_tn_ha','SPLM_Fit')]
  model_pca_splm_lag = melt(model_pca_splm_lag,id = c('years_id','district'))
  model_pca_splm_lag$variable= as.character(model_pca_splm_lag$variable)
  model_pca_splm_lag$variable[model_pca_splm_lag$variable=='fitted']='Fitted Spatial'
   
   
```

*Figure `r paste(Figure_number)`: District Wheat Yield Estimates  (2003-2012) `r c62=Figure_number` `r Figure_number =Figure_number+1 `*

```{r Panel: OLS VS TEMPORAL PANEL VS SPATIAL PANEL, fig.width=10, fig.height=6, echo=FALSE}
# Set up color pallete 
  red = '#e41a1c'
  blue = '#377eb8'
  green = '#4daf4a'
  purple = '#984ea3'

# Plot temporal and spatial fit  ------------------------------------------

  space_time_fit = rbind(model_data_lm,model_data_pca,model_pca_splm_lag)
  space_time_fit = space_time_fit[space_time_fit$district %in% unique(model_pca_splm_lag$district),]
  space_time_fit = space_time_fit[space_time_fit$district %in% unique(model_data_lm$district),]
  space_time_fit$variable= factor(space_time_fit$variable, ordered = T)
  levels(space_time_fit$variable) =c('Actual Yields','Pool Fit', 'FE Fit','Spatial Fit')
  
  ggplot()+geom_line(data=space_time_fit[space_time_fit$variable=='Actual Yields',],aes(x=years_id,y=value,colour=variable),size=1.2)+  
  geom_point(data=space_time_fit[space_time_fit$variable!='Actual Yields',],aes(x= years_id,y=value,colour=variable),alpha=0.6,size=2)+ facet_wrap( ~ district,scales = 'free' )+xlab('Year')+ylab('Wheat Tons/Hectare')+ scale_colour_discrete(name = "Legend")+
  theme(axis.text.x  = element_text(angle=0, vjust=0.5))+ scale_x_continuous(breaks=c(2005,2010,2015))
#  ggsave(paste("./F",c62,"_In_Sample_Yield_Estimates.pdf",sep=''))

```  
  
*Figure `r paste(Figure_number)`: Residuals from District Wheat Yield Estimates  (2003-2012) `r c83=Figure_number` `r Figure_number =Figure_number+1 `*
```{r Panel: OLS VS TEMPORAL PANEL VS SPATIAL PANEL HISTOGRAM, fig.width=10, fig.height=6, echo=FALSE}

# Plot temporal and spatial fit  ------------------------------------------

  residual_dist = data.frame(OLS_res = lm1$residuals,PCA_res = fixed_pca$residuals,SPLM_res = splm_lag$residuals)
  residual_dist = melt(residual_dist)
  levels(residual_dist$variable) =c('Pool', 'FE','Spatial')
   
  # ggplot(data=residual_dist,aes(x=value,group=variable))+ geom_histogram(aes(y = ..density..,fill=variable), alpha=.3, position="identity") + geom_density(aes(colour=variable),size=1.5)  
  
   ggplot(data=residual_dist,aes(x=value,group=variable))+  geom_density(aes(colour=variable,fill=variable),size=1.5,alpha=.3)+xlab('Residuals (Wheat Tons/Hectare)')+ylab('Density')+scale_color_manual('',values=c('Pool'=blue,'FE'=green,'Spatial'=purple))+scale_fill_manual('',values=c('Pool'=blue,'FE'=green,'Spatial'=purple))  
 # ggsave(paste("./F",c83,"_In_Sample_Yield_Estimates.pdf",sep=''))

```  
  
##Out-Of-Sample Cross Validation
Here we present the results of measurements of predictive accuracy outlined in the methods section. 

*Table `r paste(Table_number)`: Root Mean Squared Error (RMSE) for Out-Of-Sample Prediction (Tons/Hectare) `r t12321 =Table_number ` `r Table_number =Table_number+1 `*
```{r Out-of-Sample Estimation, fig.width=10, fig.height=6, echo=FALSE}

  #save.image("H:/Projects/India_Index_Insurance/India_Index_Insurance_Code/WriteUp/dataimage.RData")
 #NOT NEEDED #load("H:/Projects/India_Index_Insurance/India_Index_Insurance_Code/WriteUp/dataimage.RData")
  

  ####################################
  # predict to FE from SPLM
 
  # Calculate FE with dummy variables to forecast error estimations
  DM_formula_regression = yield_tn_ha ~ 0+factor(district)+ PC1 + PC2 + PC3 + PC4 
  
  splm_lag_DM <- spml(DM_formula_regression, data = pca_pred, index = c("district", "years"),
                     listw = districts_polyListw, model = "pooling", lag = T, spatial.error = "none") # must use pooling with dummy variables
 
  FE_SPLM_predict = function(model, X){
    # predict function for spatial lag FE (only specified as dummy variables)
    # add in neighborhood mean values 
    lagit = function(data,wmatrix){lag.listw(mat2listw(wmatrix),data)} # calc neighborhood mean 
    holder = list() # for non-demean
    for(year in unique(X$years)){holder = c(holder,lagit(X$yield_tn_ha[X$years==year],Wneigh))}
    X$neighbors_mn = unlist(holder)
    ### EXTRACT RELEVANT DATA AND APPLY COEFFICIENTS
    form = as.character(DM_formula_regression)                             #extract formula used
    form = paste(form[2],form[1],'0+',form[3],"+neighbors_mn",sep=" ")   #append nb density remove constant
    Xs  = model.matrix( as.formula(form) , X)                     # get all relevant data
    Bs  = as.vector(c(model$coefficients , model$arcoef))    # get coefficients move spatial coef to right
    XBs = sweep(Xs,MARGIN=2,Bs,`*`)                                         # apply coefficients to demeaned Xs
    P1  = as.data.frame(rowSums(XBs))                                       # non-spatial yhat
    return(P1$`rowSums(XBs)`)
  }
  
  # out of sample RMSE for spatial lag FE
  resid_SPLM_FE = list()
  for(year in unique(pca_pred$years)){
      pca_sub = subset(pca_pred, years != year)
      splm_lag_DM_sub <- spml(DM_formula_regression, data = pca_sub, index = c("district", "years"),
                       listw = districts_polyListw, model = "pooling", lag = T, spatial.error = "none")
      pca_pred$Yhat = FE_SPLM_predict(splm_lag_DM_sub, pca_pred)
      pca_sub = subset(pca_pred, years == year) # limit to omitted year
      resid_SPLM_FE = c(resid_SPLM_FE,pca_sub$yield_tn_ha-pca_sub$Yhat)# calc resid
  }
  # print RMSE
  RMSE_SPLM_FE = sqrt(sum((unlist(resid_SPLM_FE))^2)/length(resid_SPLM_FE))
  

  ####################################
  # predict to FE from PLM
  # formula for FE no ommited variable
  DM_PCA_formula_regression = yield_tn_ha ~ 0+factor(district)+rcs(PC1, 4) + rcs(PC2, 4) + rcs(PC3, 4) + rcs(PC4, 4) 
   
   fixed_pca2 <- plm(DM_PCA_formula_regression, data=pca_sub, index=c("district", "years"), model="pooling") # must run as pooling 

  
  FE_PLM_predict = function(model, X){
    # predict function for plm FE (only specified as dummy variables)
    ### EXTRACT RELEVANT DATA AND APPLY COEFFICIENTS
    form = as.character(formula(model))                             #extract formula used
    form = paste(form[2],form[1], form[3] ,sep=" ")   #append nb density remove constant
    Xs  = model.matrix( as.formula(form) , X)                     # get all relevant data
    Bs  = as.vector(c(model$coefficients  ))    # get coefficients move spatial coef to right
    XBs = sweep(Xs,MARGIN=2,Bs,`*`)                                         # apply coefficients to demeaned Xs
    P1  = as.data.frame(rowSums(XBs))                                       # non-spatial yhat
    return(P1$`rowSums(XBs)`)
  }

  # out of sample RMSE for spatial lag FE
  resid_fixed = list()
  for(year in unique(pca_pred$years)){
      pca_sub = subset(pca_pred, years != year)
      fixed_pca_sub <- plm(DM_PCA_formula_regression, data=pca_sub, index=c("district", "years"), model="pooling") # must run as pooling 
      pca_pred$Yhat = FE_PLM_predict(fixed_pca_sub, pca_pred )
      pca_sub = subset(pca_pred, years == year)  
      resid_fixed = c(resid_fixed,pca_sub$yield_tn_ha-pca_sub$Yhat) 
  }
  # print RMSE
  RMSE_fixed = sqrt(sum((unlist(resid_fixed))^2)/length(resid_fixed))
  
  
  ####################################
  # predict to LM
  
  # out of sample RMSE for spatial lag FE
  resid_lm = list()
  for(year in unique(pca_pred$years)){
      pca_sub = subset(pca_pred, years != year)
      lm1_sub <- lm(PCA_formula_regression, data=pca_sub)
      pca_pred$Yhat = predict(lm1_sub, pca_pred )
      pca_sub = subset(pca_pred, years == year)  
      resid_lm = c(resid_lm,pca_sub$yield_tn_ha-pca_sub$Yhat) 
  }
  # print RMSE
  RMSE_lm = sqrt(sum((unlist(resid_lm))^2)/length(resid_lm))
  
  # present results

  RMSE_table = data.frame(`Within Sample RMSE`=c(RMSE_lm1,RMSE_pca,RMSE_lag),`Out of Sample RMSE`=c(RMSE_lm,RMSE_fixed,RMSE_SPLM_FE),`Within R2`=c(Witin_R2_pool,Witin_R2_FE,Witin_R2_splm))
  row.names(RMSE_table) = c('Pool Fit','FE Fit','Spatial Fit')
  RMSE_table = as.data.frame(apply(RMSE_table,2,FUN=function(x){round(x,2)}))
  panderOptions('table.split.table', Inf)
  panderOptions('big.mark', ",")
  panderOptions('keep.trailing.zeros', F)
  pander(RMSE_table, justify = c('left', 'center','center','center'))
     
```
#Discussion
##Algorithms
A broad suite of algorithms for crop modeling using the open-source language R are provided in the Functions section of Appedix **A** with the resulting variables described in Table `r paste(VarTableNum)`. These funcitons have been designed to capture phenologically relevant components of the a growing season. They are designed to be flexibly applied to a wide set of geographic areas and crop seasons. For instance, similar or better results to those presented here were created for rice in Punjab and Haryana. The functions are also able to reasonably estimate critical management indicators such as planting and harvest dates by mixing expert opinion with optimization routines. 

Due to the idosyncratic and heterogenous nature of crop production in many developing countries these statistics are calculated independently for each area of interest and time-period. Therefore, metrics can accurately reflect spatial and temporal variability due to differnces in management practicies, climatic zone, weather, and edaphic properties. Importantly, the spatio-temporal characteristics of the data are maintained for use in panel econometic applications (looking at individual AOIs over time).

##Principal Component Loadings
Here we use principal components to avoid issues of multicolinearity across NDVI metrics for both the Rabi and Kharif seasons. An unusual feature of this study is the explicit integration of Kharif season (typically rice) for Rabi Wheat estimation. These variables were included because nutrient and water availability, although rarely modeled as such, is part of a longer-term integrative process. This process is a function of not only water availability during growing season but an integrative function of rainfall, temperatures, windspeeds, and edaphic properties, closer related to metrics such as climatic water deficit [@mann2016incorporating]. For instance, loamy soils will retain greater quantities of moisture over longer time-periods. 

Considering the longer-term nature of moisture availability, we also must consider the inability of Rabi NDVI to capture all meaninful characteristics of the season. Rainfall, especially in rich soils, in the months leading up to a growing season will be important for maintaining soil moisture across the growing season, but is particularly important during the germination process, when the NDVI signal is minimal. Looking at Figure `r paste(c62)` we can see that effects of a failed monsoon in 2004-2005, despite the extensive use of irrigation in Pubjab and Haryana, deficits in soil moisture will be difficult to make up for. 

##Regression Results
Preliminary tests from preliminary tests rule out the use of pooled OLS in favor of fixed effects methods. Looking at Table `r paste(t632)` results, PC1 which largely comprises comparisons of the current years mean value and the historic 95th percentile, we can see that low current values relative to the upper percentiles is significantly correlated with reductions in Rabi wheat yeilds (p<`r paste(round(summary(fixed_pca)[1]$coefficients[1,4],6))`). For PC2 we see that increases in the minimum values of NDVI correspond to significantly higher wheat yields (p<`r paste(round(summary(fixed_pca)[1]$coefficients[2,4],8))`). For PC3 we see that increases in season length correspond (although with a relatively small response) to significantly higher yields .  (p<`r paste(round(summary(fixed_pca)[1]$coefficients[3,4],3))`). For PC4, which also largely compares current year metrics (largely AUC values) to upper parts of the observed distribution, we see similar to PC1 significant decreases in yields (p<`r paste(round(summary(fixed_pca)[1]$coefficients[4,4],3))`). The spatial lag model follows a similar pattern of sign and significance, however the relative size of the coefficients are smaller due to the influence of $\lambda$, the spatial autoregressive coefficient (Table `r paste(t098)`). Here the large and possitive coefficient indicates strong spatial spillover effects, with uncontroled factors affecting yield not easily captured by NDVI (e.g. pest, disease, management) influencing their neighboring district.   

##Spatio-Temporal Model Accuracy 
Metrics for wheat yeild prediction at the district level are promising, with within-$R^{2}$ `r paste(range(round(Witin_R2_pool,2),round(Witin_R2_FE,2),round(Witin_R2_splm,2)), collapse=' to ')` and out of sample RMSEs as low as `r paste(min(round(RMSE_lm,2),round(RMSE_fixed,2),round(RMSE_SPLM_FE,2)), collapse=' to ')` metric tons per hectare using only information extracted from NDVI (Table `r paste(t12321)`).  The improvements in within and out of sample accuracy from leveraging the spatio-temporal properties of panel data is apparent with predictive accurancy increasing nearly `r round(RMSE_lm/RMSE_SPLM_FE,0)` fold relative to traditional OLS approaches. We can also see the improvements of accuracy obtained through integration of spatially lagged values of yeilds ($W_{ij}Y_{jt}$) are included in the regression. Note however that the inclusion of $Y_{jt}$ on the right side of the equation precludes true forecasting applications because it requires contemporaneous information on yields in neighboring districts. Note however that this particular issue could be at least partially addressed by integrated spatially and lagged values of Y for neighbors *j* ($Y_{jt-1}$), which require no contemporaneous information. 

These differences are clearly exhibited in Figure `r paste(c62)` where the fixed effect and spatial lag models clearly out perform the pooled OLS methodology. This is particularly true during years exhibiting high variance such as 2005 and 2011 where unusually low and high values are prevalent across the sample. This characteristic of panel data reflects the underlying principal of the methodology which is able to model both the time-series properities of a single district, as well as the cross-sectional (spatial) properties of a particular year. These improvements are also expressed in the distribution of residuals presented in Figure `r paste(c83)`.   

 




lm1
summary(fixed_pca)
summary(splm_lag)

#Conclusions


 

```{r pressure, echo=FALSE}
# plot(pressure)
# library(stargazer)
# stargazer(yield_ndvi[,8:10] ,  title="Descriptive statistics", digits=2,summary = T,font.size='small')

```
 

#Appendix A
##Yield Data
*Table A`r paste(Appendix_Table_number)`: Rabi Season Wheat Yields Metric Tons per Hectare by State `r Appendix_Table_number =Appendix_Table_number+1 `*
```{r, include=T,echo=F,warning=F}
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', F)
state_yeilds = ddply(yield_ndvi,~state+district,summarise,Min=min(yield_tn_ha,na.rm=T),Mean=mean(yield_tn_ha,na.rm=T),Max=max(yield_tn_ha,na.rm=T))
names(state_yeilds)[1]="State"
pander(state_yeilds, justify = c('left', 'left', 'center','center', 'center'))
```

##Principal Components Analysis
*Table A`r paste(AT1)`: Importance of PCA components*

```{r Panel:  PCA output  , echo=FALSE}
   summary(pca) 
```



##Functions
*Functions `r paste(A)`: Planting/Harvest date functions*
```{r, include=T,echo=T,warning=F}
PlantHarvestDates = function(start_date,end_date,PlantingMonth,PlantingDay,HarvestMonth,HarvestDay){
    # this function takes in date range and returns planting and harvest date for time series as a data.frame 
    # for all years of interest. Handles growing periods overlaping a new year properly.
    # NOTE: This is used to create dataframe of planting / harvest dates for many other functions
    #  
    # e.g. PlantHarvest = PlantHarvestDates('2002-01-01','2016-02-02',PlantingMonth=11, PlantingDay=23,HarvestMonth=4,HarvestDay=30)
     
    start_end_years = c(strptime(start_date,'%Y-%m-%d'),strptime(end_date,'%Y-%m-%d'))
    names(unclass(start_end_years[1]))
    start_end_years[1]$mon=PlantingMonth-1
    start_end_years[1]$mday=PlantingDay
    planting = as.Date(seq(start_end_years[1],
      length=strptime(dates[2],'%Y-%m-%d')$year-strptime(dates[1],'%Y-%m-%d')$year,
      by='year'))
    # set harvest
    start_end_years[2]$year=start_end_years[1]$year+1    # set year equal to start year +1
    start_end_years[2]$mon=HarvestMonth-1
    start_end_years[2]$mday=HarvestDay
    harvest = as.Date(seq(start_end_years[2],
      length=strptime(end_date,'%Y-%m-%d')$year-strptime(start_date,'%Y-%m-%d')$year,
      by='year'))
    return(data.frame(planting=planting,harvest=harvest))
  }

SearchMinumumBeforeAfterDOY = function(x,dates_in,DOY_in,days_shift,dir){
  	# calculates the global minimum for days before,after,both of expected planting date
    # best to set DOY as the last expected date of planting
    # x = vegetation index, dates_in = dates of observation POSIX, DOY_in = expected planting or harvest date
    # days_shift = # days to search around DOY_in,  dir='before' 'after' 'beforeafter'
  
  	if(days_shift<=8){print('Using less than 8 days is dangerous, 15-30 stable')}
  
  	# avoid problems with time class
  	if(is.na(DOY_in[1])){print('ERROR: convert date format to %Y%j');break}
  	if(class(dates_in)[1]!= 'POSIXlt' ){dates_in=as.POSIXlt(dates_in)}
  
  	# limit to fixed # of days before/after DOY
      DOY_in = as.POSIXlt(DOY_in)
      DOY_before = DOY_in
  
  	#names(unclass(DOY_before[1]))
  	if(dir=='before') DOY_before$mday=DOY_before$mday-days_shift      # set days before to doy - days_before
  	if(dir=='after') DOY_before$mday=DOY_before$mday+days_shift      # set days before to doy - days_before
      if(dir=='beforeafter'){ DOY_before$mday=DOY_before$mday-days_shift 
        DOY_in$mday=DOY_in$mday+days_shift}
  	DOY_table = data.frame(DOY_before=DOY_before,DOY_in=DOY_in)   #join start end search dates
  
    # list all days 'days_before' DOY_in
     if(dir=='before'|dir=='beforeafter'){ DOY_interest = as.POSIXlt(unlist(lapply(1:dim(DOY_table)[1],
  	  function(h){format(seq(DOY_table[h,1],
                DOY_table[h,2],by='day'),'%Y-%m-%d')})),tz='UTC')}
    if(dir=='after'){DOY_interest = as.POSIXlt(unlist(lapply(1:dim(DOY_table)[1],
  	  function(h){format(seq(DOY_table[h,2],
                DOY_table[h,1],by='day'),'%Y-%m-%d')})),tz='UTC')}
  
    # find all local minima, and match with DOY
    x_DOY_interest = x[dates_in %in% DOY_interest]
    dates_DOY_interest = dates_in[dates_in %in% DOY_interest]
    # get min value for this period for each year
    sort(AnnualMaxima(x_DOY_interest*-1,as.Date(dates_DOY_interest)))
}
```

*Functions `r paste(B)`: Flexible growing season vegetation metrics*
```{r, include=T,echo=T,warning=F}
PeriodAggregator = function(x,dates_in,date_range_st, date_range_end,by_in='days',FUN){
    	# returns a summary statistic of x for any function FUN, over the period defined by date_range_st, date_range_end
      # x = vegetation index data, dates_in = dates of observation POSIX, dates_in,date_range_st = start end dates of period, FUN = function
      # E.g. PeriodAggregator(x=plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates,date_range_st=plotdatasmoothed$dates[1],date_range_end=plotdatasmoothed$dates[20], FUN = function(y){mean(y,na.rm=T)})
    	if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)
    	if(class(date_range_st)[1]== "POSIXct" ){date_range_st = as.Date(date_range_st)
                                             date_range_end = as.Date(date_range_end)}
    	#Avoid problems with missing plant or harvest dates
    	if(length(date_range_st)!=length(date_range_end)){print('number of elements in start end dates dont match');	break}
    	dataout=lapply(1:length(date_range_st),function(z){
      		DateRange = seq(date_range_st[z],date_range_end[z],by=by_in)
      		x=x[dates_in %in% DateRange]
      		dates_in=dates_in[dates_in %in% DateRange]
      		FUN(x)})
    	dataout = do.call(c,dataout)
      names(dataout)=format(date_range_st,'%Y')
  	  dataout
  }
```

*Functions `r paste(C)`: Area under the curve estimation - smoothing splines*
```{r, include=T,echo=T,warning=F}
PeriodAUC = function(x_in,dates_in,DOY_start_in,DOY_end_in){
         # calculate area under the curve by period of the year using spline estimation
         # x = data, dates_in=asDate(dates),DOY_start_in=asDate(list of start periods),DOY_end_in=asDate(list of end per
         # x = plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates , DOY_start_in= plant_dates ,DOY_end_in=harvest_dates)
        if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)
         dates_group = rep(0,length(dates_in))    # create storage for factors of periods
         # get sequences of periods of inerest
         seq_interest = lapply(1:length(DOY_start_in),function(z){seq(DOY_start_in[z],DOY_end_in[z],by='days')})
         # switch dates-group to period group
         years_avail = sort(as.numeric(unique(unlist(
                lapply(seq_interest,function(z) format(z,'%Y'))))))
         for(z in 1:length(seq_interest)){        #assigns year for beginging of planting season
		            dates_group[dates_in %in% seq_interest[[z]]]=years_avail[z]
                assign('dates_group',dates_group,envir = .GlobalEnv) }  # assign doesn't work in lapply using for loop instead
	      # calculate AUC for periods of interest
         FUN = function(q,w){auc(q,w,type='spline')}
         datesY = format(dates_in,'%Y')
         data.split = split(x_in,dates_group)
         d = do.call(c,lapply(2:length(data.split),function(z){   # start at 2 to avoid group=0
	          	FUN(q=1:length(data.split[[z]]),w=data.split[[z]]) }))
         names(d) = names(data.split)[2:length(data.split)]
         d
	}
```
*Functions `r paste(D)`: Area under the curve estimation - trapazoidal estimation*
```{r, include=T,echo=T,warning=F}
PeriodAUC_method2 = function(x_in,dates_in,DOY_start_in,DOY_end_in){
	        #NOTE SPLINE METHOD 1 SEEMS to WORK BETTER
         # calculate area under the curve by period of the year
         # x = data, dates_in=asDate(dates),DOY_start=asDate(list of start periods),DOY_end=asDate(list of end per$
         # x = plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates , DOY_start=annualMinumumBeforeDOY(x = plotd$
         if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)

         dates_group = rep(0,length(dates_in))    # create storage for factors of periods
         # get sequences of periods of inerest
         seq_interest = lapply(1:length(DOY_start_in),function(z){seq(DOY_start_in[z],DOY_end_in[z],by='days')})
         # switch dates-group to period group
         years_avail = sort(as.numeric(unique(unlist(
                lapply(seq_interest,function(z) format(z,'%Y'))))))
         for(z in 1:length(seq_interest)){        #assigns year for beginging of planting season
                dates_group[dates_in %in% seq_interest[[z]]]=years_avail[z]
                assign('dates_group',dates_group,envir = .GlobalEnv) }  # assign doesn't work in lapply using for loop instead
 
        # calculate AUC for periods of interest
         FUN = function(q,w){  sum(diff(q)*rollmean(w,2))}
         datesY = format(dates_in,'%Y')
         data.split = split(x_in,dates_group)
         d = do.call(c,lapply(2:length(data.split),function(z){   # start at 2 to avoid group=0
                FUN(q=1:length(data.split[[z]]),w=data.split[[z]]) }))
         names(d) = names(data.split)[2:length(data.split)]
         #print(cbind(names(data.split)[2:length(data.split)], d))
         d
        }
```

*Functions `r paste(E)`: Base function used for estimating sample quantiles*
```{r, include=T,echo=T,warning=F}
quantile_type8 = function(x){
  quantile(x ,p=Quant_percentile,type=8,na.rm=T)
}
```

*Functions `r paste(F)`: Function to return date of any given phenomenon*
```{r, include=T,echo=T,warning=F}
PeriodAggregatorDates = function(x,dates_in,date_range_st, date_range_end,by_in='days',FUN){
        # returns a date of summary statistic defined by FUN
        # like the date of the maximum value of x for the period defined by date_range_st, date_range_end
        # other parameters identical to other functions show above
        if(class(dates_in)[1]== "POSIXct"|class(dates_in)[1]== "POSIXlt" )dates_in = as.Date(dates_in)
        if(class(date_range_st)[1]== "POSIXct" ){date_range_st = as.Date(date_range_st)
                                             date_range_end = as.Date(date_range_end)}
        #Avoid problems with missing plant or harvest dates
        if(length(date_range_st)!=length(date_range_end)){print('number of elements in start end dates dont match');break}

        dataout=lapply(1:length(date_range_st),function(z){
            DateRange2 = seq(date_range_st[z],date_range_end[z],by=by_in)
            x2 = x[dates_in %in% DateRange2]
            dates_in2 = dates_in[dates_in %in% DateRange2]
            which_max = which(FUN(x2) ==  x2)
         		if(length(which_max)>1){
          		    which_max = c(which_max[1],which_max[length(which_max)]) # limit to only 2 
          			if((which_max[2]-which_max[1])==1){
          				which_max=which_max[1]  # favor the first instance of maximum
          			  } else if((which_max[2]-which_max[1])==2){
          				which_max=which_max[1]+1 # is seperated by 2 choose middle left
                  } else if((which_max[2]-which_max[1])==3){
          				which_max=which_max[1]+2} # is seperated by 3 choose middle
        		}
            max_dates = dates_in2[which_max]
                })
        dataout = do.call(c,dataout)
        names(dataout)=format(date_range_st,'%Y')
        dataout
    }
```

*Functions `r paste(G)`: Mean day of the year values*
```{r, include=T,echo=T,warning=F}
  AnnualAverageDOYvalues = function(x,dates_in){
    	# calculates the average value for DOY for the whole series
    	datesj = format(dates_in,'%j')
    	do.call(c,lapply(split(x,datesj),function(y){mean(y,na.rm=T)}))}
```

*Functions `r paste(H)`: Smoothing splines with outlier removal*
```{r, include=T,echo=T,warning=F}
#---------------------------------------------------------------------
# This function takes a time series w/ dates (x, dates) and returns a spline smoothed time series with outliers removed.
# Outliers are identified as points with absolute value more than out_sigma * sd, where sd is the residual
# standard deviation between the input data and the initial spline fit, and out_sigma is a variable
# coefficient. The spline smoothing parameter spline_spar controls the smoothness of the fit (see spline.smooth help)
# and out_iterations controls the number of times that outliers are checked and removed w/ subsequent spline refit
# pred_dates is a vector of dates where spline smoothed predictions of x are desired. If NA, then a daily series spanning
# min(dates)-max(dates) is returned
SplineAndOutlierRemoval <- function(x, dates, out_sigma=3, spline_spar=0.3, out_iterations=1,pred_dates){
  dates <- as.numeric(dates) # spline doesn't work with dates
  pred_dates = as.numeric(pred_dates)
  # if prediction dates aren't provided, we assume we want daily ones
  if(is.na(pred_dates[1])){
    pred_dates <- min(dates, na.rm=T):max(dates, na.rm=T)}
  # eliminate outliers and respline
  for(i in 1:out_iterations){
    # fit a smoothing spline to non-missing data
    spl <- try(smooth.spline(dates[!is.na(x)], x[!is.na(x)], spar=spline_spar), silent=T)
    if(inherits(spl, 'try-error')){
      print("Failed to fit smoothing spline")
      return(NA)
    }
    smooth_x <- try(predict(spl, dates)$y, silent=T) # calculate spline smoothed values
    if(inherits(smooth_x, 'try-error')){
      print("Failed to predict with spline")
      return(NA)
    }
    smooth_x_resid <- x - smooth_x # calculate residuals from spline
    smooth_x_resid_sd <- try(sd(smooth_x_resid, na.rm=T), silent=T) # standard dev of absolute value of residuals
    if(inherits(smooth_x_resid_sd, 'try-error')){
      print("Failed to get sd of residuals")
      return(NA)
    }
    outliers <- abs(smooth_x_resid) > out_sigma * smooth_x_resid_sd
    outliers[is.na(outliers)] <- F
    if(sum(outliers) > 0){
      # if we found outliers, eliminate them in x and refit up to iterations
      x[outliers] <- NA
    }else{
      # if we didn't find any outliers, we abandon the iteration and return the smoothed values
      smooth_x_return <- try(predict(spl, pred_dates)$y, silent=T)
      if(inherits(smooth_x_return, 'try-error')){
        print("No outliers, but failed to predict with final spline")
        return(NA)
      }else{
        return(smooth_x_return)
      }
    }
  }
  # fit the spline to the outlier screened data, then return the predicted series
  spl <- try(smooth.spline(dates[!is.na(x)], x[!is.na(x)], spar=spline_spar), silent=T)
  if(inherits(spl, 'try-error')){
    print("Failed to predict with final spline")
    return(NA)
  }else{
    smooth_x_return <- try(predict(spl, pred_dates)$y, silent=T)
    if(inherits(smooth_x_return, 'try-error')){
      return(NA)
    }else{
      return(smooth_x_return)
    }
  }
}
```

*Function `r paste(I)`: Flexible annual vegetation metrics*
```{r, include=T,echo=T,warning=F}
  AnnualAggregator = function(x,dates_in,FUN){
    # returns an annual summary statistic of any function
    # x = vegetation index data, dates_in = dates of observation POSIX,
    # E.g. AnnualAggregator(x=  plotdatasmoothed$EVI,dates_in = plotdatasmoothed$dates, FUN = function(y){mean(y,na.rm=T)})
    datesY = format(dates_in,'%Y')
    do.call(c,lapply(split(x,datesY),FUN))}
```

*Function `r paste(J)`: Rapid multicore extract raster data by point or polygon*
```{r, include=T,echo=T,warning=F}
extract_value_point_polygon = function(point_or_polygon, raster_stack, num_workers){
          # Returns list containing values from locations of spatial points or polygons
 	  # if polygons are too small reverts to centroid 
          if(class(raster_stack)!='list'){raster_stack=list(raster_stack)}
          lapply(c('raster','foreach','doParallel'), require, character.only = T)
          registerDoParallel(num_workers)
          ptm <- proc.time()
          # iterate between points or polygons
          ply_result = foreach(j = 1:length(point_or_polygon),.inorder=T) %do%{
                print(paste('Working on feature: ',j,' out of ',length(point_or_polygon)))
                get_class= class(point_or_polygon)[1]
                # switch rasterstack according to which point or polygon is %over%
                for(z in 1:length(raster_stack)){
                        # set raster to use
                        raster_stack_use = raster_stack[[z]]
                        # get cell numbers of point of polygon, repeat if missing
                        if(get_class=='SpatialPolygons'|get_class=='SpatialPolygonsDataFrame'){
                            cell = as.numeric(cellFromPolygon(raster_stack_use, point_or_polygon[j,], weights=F)[[1]])
                            # if polygon is too small to find cells, convert to centroid and get cellfromXY
                           if(length(cell)==0){                                       #coord(poly) returns centroid
                                cell = as.numeric(na.omit(cellFromXY(raster_stack_use, coordinates(point_or_polygon[j,]) )))}}
                        if(get_class=='SpatialPointsDataFrame'|get_class=='SpatialPoints'){
                            cell = as.numeric(na.omit(cellFromXY(raster_stack_use, point_or_polygon[j,])))}
                        # if cells found keep raster_stack_use = raster_stack[[z]]
                        if(length(cell)!=0){break}
                        # if cells not found repeat for different stack or return NA
                        if(length(cell)==0 & z!=length(raster_stack)){next}else{return(NA)}
                }
                # create raster mask from cell numbers
                r = rasterFromCells(raster_stack_use, cell,values=F)
                result = foreach(i = 1:dim(raster_stack_use)[3],.packages='raster',.inorder=T) %dopar% {
                   crop(raster_stack_use[[i]],r)
                }
                result=as.data.frame(getValues(stack(result)))
                return(result)
          }
          print( proc.time() - ptm)
          endCluster()
          return(ply_result)
 }
```


References {#references .unnumbered}
==========



*Figure `r paste(Figure_number)`: Fitted vs actual for estimation of equation (`r paste(Formula_number)`)  `r aaa=Figure_number` `r Figure_number =Figure_number+1 `*
```{r Panel: Fitted vs actual PCA PLOT, fig.width=10, fig.height=6, echo=FALSE}
# COMMENT OUT TOP NOT SURE WHAT DOES 
 # get prediction and and model.frame 
  # fitted_pca = data.frame(PCA_Fit = fixed_pca$model[[1]] - fixed_pca$residuals)
  # model_data_pca = cbind(as.data.frame(as.matrix(fixed_pca$model)),fitted_pca)
  # model_data_pca = cbind(model_data_pca,na.omit(model.frame(PCA_formula_regression_dataframe,pca_pred)))  
  # model_data_pca$district = as.character(model_data_pca$district)
  # model_data_pca$years_id = as.numeric(substr(model_data_pca$year,1,4))
  # model_data_pca = model_data_pca[,c('district','years_id','yield_tn_ha','PCA_Fit')]
  # model_data_pca = melt(model_data_pca,id = c('years_id','district'))
  
  #ggplot(data=model_data_pca,aes(x=as.factor(years_id),y=value,colour=variable,alpha=0.5))+
  #  geom_point(size=2) + facet_wrap( ~ district )+xlab('Year')+ylab('Wheat Tons / ha')+ theme(legend.position="none")+ 
  #  theme(axis.text.x  = element_text(angle=90, vjust=0.5))
  
# plots of model performance 
#ggplot(data=melt(data.frame(Residuals = fixed_pca$residuals,Yield_tn_ha=fixed_pca$model[[1]],Prc_Error=fixed_pca$residuals/fixed_pca$model[[1]])),aes(x=value,fill=variable))+geom_histogram(bins=50)

#ggplot(data= data.frame(Prc_Error=fixed_pca$residuals/fixed_pca$model[[1]]*100),aes(x=Prc_Error))+geom_histogram(bins=30)+xlab('% Error')


# OMITING BC OF ERROR not sure what this is used for. 

  # error_pca = data.frame(PCA_Error = fixed_pca$residuals)
  # error_data_pca = cbind(error_pca,na.omit(model.frame(PCA_formula_regression_dataframe,pca_pred)))  
  # error_data_pca$Per_Error =  (error_data_pca$PCA_Error/ error_data_pca$yield_tn_ha)*100
  # error_data_pca$years_id = as.numeric(substr(error_data_pca$year,1,4))
  # error_data_pca$district = as.character(error_data_pca$district)
  # error_data_pca = error_data_pca[,c('district','years_id','Per_Error')]
  # error_data_pca = melt(error_data_pca,id = c('years_id','district'))

 
# histogram of errors stacked histogram use in presentation
  # library(plotly)
  # ggplot(data=error_data_pca,aes(x=value,fill=district))+ geom_histogram( )+xlab('% Error')+ theme(legend.position="none")
  # ggplotly()
  # 
  # 
  # ggplot(data= data.frame(Prc_Error=abs(fixed_pca$residuals/fixed_pca$model[[1]]*100)),aes(x=Prc_Error)) +   stat_ecdf(geom = "step",size=1)+xlab('Absolute value of % error')+ylab('Cumulative Density')
  # 
  # ggplot(data= data.frame(Prc_Error=abs(fixed_pca$residuals/fixed_pca$model[[1]]*100)),aes(x=Prc_Error)) + stat_ecdf(geom = "step",size=1)+xlab('Absolute value of % error')+ylab('Cumulative Density')


# 
# mean_neighbors_panel <- function(values,id,years_in,sweights){
#         # function calculates the mean value of neighbors values by using sweights e.g. Wneigh = nb2mat(out, style='W')
#         # ( data must be sorted by year, id )
#      
#         mean_values=list()  # place to store outputs
#         for(i in unique(years_in)){
#             value = subset(values, years_in == i) # limit to year
#             for(row in 1:(length(values)/length(unique(years_in)))   ){
#                 mean_values=c(mean_values,weighted.mean(value,w = sweights[row,])) #calc neighbor values
#             }
#         }
#         return(as.numeric(mean_values))
# }
```


<!-- ###Cross-sectional Analysis -->
<!-- ```{r Cross-Section: Fitted vs actual PCA estimate, echo=FALSE} -->
<!--   cs_formula = yield_tn_ha~ PC1+PC2+PC3+PC4+PC5  -->
<!--   lm_cs <- lm(cs_formula, data=pca_pred[as.numeric(pca_pred$years)==12,]) -->
<!--   lm_pred_cs = predict(lm_cs,newdata = pca_pred,se.fit=T) -->
<!--   R2 = summary(lm_cs)$r.squared -->
<!--   R2adj = summary(lm_cs)$adj.r.squared -->

<!--   # calculate within R2 http://forums.eviews.com/viewtopic.php?t=4709 -->
<!--   SSR_FULL = sum((pca_pred$yield_tn_ha-lm_pred_cs$fit)^2) -->
<!--   SSR_FE = sum( lm(yield_tn_ha ~ 1 +as.factor(district) , data=pca_input)$residuals^2) -->
<!--   Witin_R2 =  1 - (SSR_FULL/SSR_FE)   -->
<!--   print(paste(round(R2,2),round(R2adj,2),round(R2,2))) -->

<!-- ``` -->


